[
  {
    "objectID": "statistical_hypotheses.html",
    "href": "statistical_hypotheses.html",
    "title": "Statistical Hypotheses",
    "section": "",
    "text": "This page features theories involving statistical hypotheses, including:"
  },
  {
    "objectID": "statistical_hypotheses.html#classic-jury-analogy",
    "href": "statistical_hypotheses.html#classic-jury-analogy",
    "title": "Statistical Hypotheses",
    "section": "Classic Jury Analogy",
    "text": "Classic Jury Analogy\nConsider a jury in a criminal trial. When a defendant is accused of a crime, the jury presumes that they are not guilty.\n\n\\(H_0\\): Not Guilty\n\\(H_A\\): Guilty\n\nThe jury is then presented with evidence. If the evidence seems implausible under the assumption of non-guilt, we might reject the null hypothesis of non-guilt, and claim that the defendant is (likely) guilty.\n\nIn the case of statistical hypothesis testing, we use data to find evidence, leading us to arrive at two possible conclusions:\n\n\nReject the null hypothesis, \\(H_0\\), in favor of the alternative hypothesis, \\(H_A\\)\nFail to reject the null hypothesis, \\(H_0\\)"
  },
  {
    "objectID": "statistical_hypotheses.html#setting-up-the-hypothesis-test",
    "href": "statistical_hypotheses.html#setting-up-the-hypothesis-test",
    "title": "Statistical Hypotheses",
    "section": "Setting Up the Hypothesis Test",
    "text": "Setting Up the Hypothesis Test\nThe null hypothesis will almost always be represented with equivalence. The alternative hypothesis will be then be represented with an inequality or shown to be not equivalent.\n\n\\(H_0\\): \\(\\theta = \\theta_0\\)\n\\(H_A\\): One of the Following:\n\n\\(\\theta &gt; \\theta_0\\)\n\\(\\theta &lt; \\theta_0\\)\n\\(\\theta \\neq \\theta_0\\)\n\n\nThe alternative hypothesis is the hypothesis for which we are seeking statistical evidence for."
  },
  {
    "objectID": "statistical_hypotheses.html#rejection-regions-for-alternative-hypotheses",
    "href": "statistical_hypotheses.html#rejection-regions-for-alternative-hypotheses",
    "title": "Statistical Hypotheses",
    "section": "Rejection Regions for Alternative Hypotheses",
    "text": "Rejection Regions for Alternative Hypotheses\nThe following example uses the \\(z\\) statistic, but the concept holds for other statistics and critical values.\n\n\n\n\n\n\n\nAlternative Hypothesis\nRejection Region for Level \\(\\alpha\\) Test\n\n\n\n\n\\(H_A\\): \\(\\theta &gt; \\theta_0\\)\n\\(z \\geq z_{\\alpha}\\)\n\n\n\\(H_A\\): \\(\\theta &lt; \\theta_0\\)\n\\(z \\leq z_{\\alpha}\\)\n\n\n\\(H_A\\): \\(\\theta \\neq \\theta_0\\)\n\\(z \\geq z_{\\alpha}\\) OR \\(z \\leq z_{\\alpha}\\)"
  },
  {
    "objectID": "statistical_hypotheses.html#type-i-error",
    "href": "statistical_hypotheses.html#type-i-error",
    "title": "Statistical Hypotheses",
    "section": "Type I Error",
    "text": "Type I Error\nA Type I error is rejecting the null hypothesis when it is actually true. This is actually representing by \\(\\alpha\\), where significance level is set at \\(100*(1-\\alpha)\\)."
  },
  {
    "objectID": "statistical_hypotheses.html#type-ii-error",
    "href": "statistical_hypotheses.html#type-ii-error",
    "title": "Statistical Hypotheses",
    "section": "Type II Error",
    "text": "Type II Error\nA Type II error is failing to reject the null hypothesis when it is actually false. This is normally considered worse than Type I errors."
  },
  {
    "objectID": "statistical_hypotheses.html#visualizing-errors",
    "href": "statistical_hypotheses.html#visualizing-errors",
    "title": "Statistical Hypotheses",
    "section": "Visualizing Errors",
    "text": "Visualizing Errors"
  },
  {
    "objectID": "statistical_hypotheses.html#power",
    "href": "statistical_hypotheses.html#power",
    "title": "Statistical Hypotheses",
    "section": "Power",
    "text": "Power\nConsidering power helps to ensure high quality hypothesis tests.Recall” - Type I errors are when we reject the null hypothesis when it is true - Type II errors are when we fail to reject the null hypothesis when the alternative hypothesis is true\nThe power of a hypothesis test is the probability of making the correct decision if the alternative hypothesis is true. In other words, it is the probability of rejecting the null hypothesis when the alternative hypothesis is true.\nIf \\(\\beta\\) is the probability of making a Type II error, then\n\\(Power = 1 - \\beta\\)\nIn general, we want to minimize \\(\\alpha\\) and maximize power."
  },
  {
    "objectID": "statistical_hypotheses.html#visualizing-power",
    "href": "statistical_hypotheses.html#visualizing-power",
    "title": "Statistical Hypotheses",
    "section": "Visualizing Power",
    "text": "Visualizing Power"
  },
  {
    "objectID": "statistical_hypotheses.html#the-bootstrap-process",
    "href": "statistical_hypotheses.html#the-bootstrap-process",
    "title": "Statistical Hypotheses",
    "section": "The Bootstrap Process",
    "text": "The Bootstrap Process\n\nTake re-samples (with replacement) of the same size as the original sample.\nCompute the desired statistic for each re-sample. These statistics form the distribution.\nIf we’re specifically targeting a \\((100-\\alpha)\\%\\) confidence interval, then find the \\(\\alpha^{th}\\) and \\((1-\\alpha)^{th}\\) percentiles. This will be the CI."
  },
  {
    "objectID": "statistical_hypotheses.html#the-randomization-process",
    "href": "statistical_hypotheses.html#the-randomization-process",
    "title": "Statistical Hypotheses",
    "section": "The Randomization Process",
    "text": "The Randomization Process\nImage we have boolean results from two separate groups. In other words, within each group, some have the results of yes and some have the results of no. Between the two groups, there are different proportions of yes values. We want to test if there is an explanatory variable causing the difference in proportions, or if it is due to inherent variability.\nIn the following process, let’s consider each result a card, and each card will have a label of yes or no.\nThe groups will likely not be the same size, let’s call the length of Group 1 \\(n_1\\) and the length of Group 2 \\(n_2\\).\n\nCreating the Distribution\n\nShuffle all the cards. Group displacement will occur (i.e. after shuffling there will only be a total count of yes and no, and sense of group will be removed).\nPut \\(n_1\\) cards into Group 1 and \\(n_2\\) cards into Group 2\nCompute the different in proportions between yes for each group\nRepeat this many times to get a distribution of the differences in proportions\n\n\n\nPerforming the Hypothesis Test\nThe hypothesis test will look something of the form (alternative depends on how the test is setup):\n\n\\(H_0\\): \\(p_1 = p_2\\) (i.e. the true proportions of the group are equivalent)\n\\(H_A\\): \\(p_1 &gt; p_2\\) (i.e. the true proportions of Group 1 is greater than Group 2, or different in some manner depending on how the test is constructed)\n\nWhen performing the hypothesis test, we want to compare the critical value on rejection region with the test statistic.\nIf the test statistic is at least as extreme as the critical value calculated from the distribution (i.e. test statistic lies within the rejection region), for a specified significance level, then we can reject the null hypothesis in favor of the alternative hypothesis. If not, we lack sufficient evidence to reject the null hypothesis."
  },
  {
    "objectID": "statistical_hypotheses.html#inference-on-mean-and-proportions",
    "href": "statistical_hypotheses.html#inference-on-mean-and-proportions",
    "title": "Statistical Hypotheses",
    "section": "Inference on Mean and Proportions",
    "text": "Inference on Mean and Proportions\n\n\n\nCategory\n\\(n \\geq 30\\)\n\\(n &lt; 30\\)\n\n\n\n\nNormal Data, Known \\(\\sigma\\)\nz-test\nz-test\n\n\nNormal Data, Unknown \\(\\sigma\\)\nz-test\nt-test\n\n\nNon-Normal Data, Known \\(\\sigma\\)\nz-test\nbootstrap\n\n\nNon-Normal Data, Unknown \\(\\sigma\\)\nz-test\nbootstrap"
  },
  {
    "objectID": "statistical_hypotheses.html#inference-on-variance",
    "href": "statistical_hypotheses.html#inference-on-variance",
    "title": "Statistical Hypotheses",
    "section": "Inference on Variance",
    "text": "Inference on Variance\n\n\n\nCategory\n\\(n \\geq 30\\)\n\\(n &lt; 30\\)\n\n\n\n\nNormal Data\nchi-squared\nchi-squared\n\n\nNon-Normal Data\nfurther research required\nbootstrap"
  },
  {
    "objectID": "statistical_hypotheses.html#inference-on-anything-else-median-skew-discrimination-rate-etc.",
    "href": "statistical_hypotheses.html#inference-on-anything-else-median-skew-discrimination-rate-etc.",
    "title": "Statistical Hypotheses",
    "section": "Inference on Anything Else (Median, Skew, Discrimination Rate, etc.)",
    "text": "Inference on Anything Else (Median, Skew, Discrimination Rate, etc.)\n\n\n\nCategory\n\\(n \\geq 30\\)\n\\(n &lt; 30\\)\n\n\n\n\nNormal Data\nbootstrap\nbootstrap\n\n\nNon-Normal Data\nbootstrap\nbootstrap"
  },
  {
    "objectID": "statistical_hypotheses.html#hypothesis-testing",
    "href": "statistical_hypotheses.html#hypothesis-testing",
    "title": "Statistical Hypotheses",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing"
  },
  {
    "objectID": "statistical_hypotheses.html#bootstrapping-1",
    "href": "statistical_hypotheses.html#bootstrapping-1",
    "title": "Statistical Hypotheses",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "statistical_hypotheses.html#hypothesis-testing-with-randomization-1",
    "href": "statistical_hypotheses.html#hypothesis-testing-with-randomization-1",
    "title": "Statistical Hypotheses",
    "section": "Hypothesis Testing with Randomization",
    "text": "Hypothesis Testing with Randomization"
  },
  {
    "objectID": "statistical_hypotheses.html#hypothesis-testing-1",
    "href": "statistical_hypotheses.html#hypothesis-testing-1",
    "title": "Statistical Hypotheses",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing"
  },
  {
    "objectID": "statistical_hypotheses.html#bootstrapping-2",
    "href": "statistical_hypotheses.html#bootstrapping-2",
    "title": "Statistical Hypotheses",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nLecture 5: replace, sample, and quantile."
  },
  {
    "objectID": "statistical_hypotheses.html#hypothesis-testing-with-randomization-2",
    "href": "statistical_hypotheses.html#hypothesis-testing-with-randomization-2",
    "title": "Statistical Hypotheses",
    "section": "Hypothesis Testing with Randomization",
    "text": "Hypothesis Testing with Randomization"
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Inference Models",
    "section": "",
    "text": "This page features information about creating and interpreting regression models."
  },
  {
    "objectID": "regression.html#the-expected-value-of-the-response-variable",
    "href": "regression.html#the-expected-value-of-the-response-variable",
    "title": "Inference Models",
    "section": "The Expected Value of the Response Variable",
    "text": "The Expected Value of the Response Variable\nGiven \\(Y = \\alpha + \\beta X + \\epsilon\\), what is \\(E[Y]\\)?\n\\(E[Y] = E[\\alpha + \\beta X + \\epsilon]\\)\n\\(= E[\\alpha] + E[\\beta X] + E[\\epsilon]\\)\n\\(= \\alpha + \\beta E[X] + 0\\)\n\\(= \\alpha + \\beta X\\)"
  },
  {
    "objectID": "regression.html#interpreting-parameters",
    "href": "regression.html#interpreting-parameters",
    "title": "Inference Models",
    "section": "Interpreting Parameters",
    "text": "Interpreting Parameters\n\\(\\beta\\): the slope of our true regression line represents the increase in our response from a unit increase in our feature."
  },
  {
    "objectID": "regression.html#minimizing-the-residuals",
    "href": "regression.html#minimizing-the-residuals",
    "title": "Inference Models",
    "section": "Minimizing the Residuals",
    "text": "Minimizing the Residuals\nGiven our data, \\((x_1, y_1), \\dots, (x_n, y_n)\\) how do we minimize the residuals, \\(\\epsilon_i = y_i - (\\alpha + \\beta x_i)\\)? In other words, how do we minimize the sum of the squared errors?\n\ninsert image of vertical line from regression line to point, label it e_i\n\nGiven:\n\n\n\\(y_i\\): the actual value of the data point\n\n\\(\\hat{y_i}\\): the predicted value of the \\(i^{th}\\) data point\n\n\\(SSE = \\sum\\limits_{i=1}^{n} (y_i - \\hat{y_i})^2\\)\n\nthe point-estimates (single value estimated from the data) of the slope and intercept parameters are called the least-squares estimates, and are defined to be the values that minimize the SSE. The SSE can be thought of as a measure of how much variation in \\(Y\\) is left unexplained by the model."
  },
  {
    "objectID": "regression.html#how-we-find-the-parameter-estimates",
    "href": "regression.html#how-we-find-the-parameter-estimates",
    "title": "Inference Models",
    "section": "How we Find the Parameter Estimates?",
    "text": "How we Find the Parameter Estimates?\n\\(SSE = \\sum\\limits_{i=1}^{n} (y_i - \\hat{y_i})^2\\)\n\\(= \\sum\\limits_{i=1}^{n} (y_i - (\\alpha - \\beta x_i))^2\\)\nAfter making this substitution, compute…\n\n\\(\\frac{\\partial SSE}{\\partial \\alpha} = 0\\)\n\\(\\frac{\\partial SSE}{\\partial \\beta} = 0\\)\n\nWhich yield the following solutions, respectively…\n\n\\(\\alpha = \\bar{y} - \\beta \\bar{x}\\)\n\\(\\beta = \\frac{\\bar{xy} - \\bar{x} \\bar{y}}{\\bar{x^2} - (\\bar{x})^2}\\)"
  },
  {
    "objectID": "regression.html#estimating-the-variance",
    "href": "regression.html#estimating-the-variance",
    "title": "Inference Models",
    "section": "Estimating the Variance",
    "text": "Estimating the Variance\nGiven that the parameter \\(\\sigma^2\\) determines the spread of the data about the true regression line, our estimate of the variance is \\(\\hat{\\sigma^2}\\), which is equivalent to:\n\\(\\hat{\\sigma^2} = \\frac{SSE}{n-2}\\)"
  },
  {
    "objectID": "regression.html#coefficient-of-determination",
    "href": "regression.html#coefficient-of-determination",
    "title": "Inference Models",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\nThe coefficient of determination, \\(R^2\\), quantifies how well the model explains the data. \\(R^2\\) ranges from \\(0\\) to \\(1\\).\nThe regression sum of squares is given by:\n\\(SSR = \\sum\\limits_{i=1}^n (\\hat{y_i} - \\bar{y})^2\\)\nand gives a sense of how much variation in \\(Y\\) is explained by our model.\nA quantitative measure of the total amount of variation in observed \\(Y\\) values is given by the total sum of squares:\n\\(SST = \\sum\\limits_{i=1}^n (y_i - \\bar{y})^2\\)\nSST is what we would get if we used the mean of the data as our model.\nNote that the sum of squared deviations about the least-squares line is smaller than the sum of squared deviations about nay other line.\n\\(SSE \\leq SST\\)\n\nInclude image of the three measures.\n\n\\(R^2\\)\nTherefore, the ratio of \\(\\frac{SSE}{SST}\\) is the proportion of the total variation in the data (SST) that cannot be explained by the SLR model (SSE). So we define the coefficient of determination \\(R^2\\) to be the proportion of variance that can be explained by the model.\n\\(R^2\\) has a magnitude between \\(0\\) and \\(1\\), with the closer to \\(1\\) be the better (i.e. the higher the number the more of the variation that can be explained by the model).\n\\(R^2 = 1 - \\frac{SSE}{SST}\\)"
  },
  {
    "objectID": "regression.html#slope-distribution",
    "href": "regression.html#slope-distribution",
    "title": "Inference Models",
    "section": "Slope Distribution",
    "text": "Slope Distribution\nDistribution\n\\(\\hat{\\beta} \\sim N(\\beta, \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\)\n\\(SE(\\hat{\\beta}) = \\frac{\\sigma^2}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}}\\)\nHypothesis Tests\n\\(H_0\\): \\(\\beta = 0\\)\n\\(H_A\\): \\(\\beta \\neq 0\\)\nConfidence Intervals\n\\(\\hat{\\beta} \\pm t_{\\alpha / 2, df = n-2} SE(\\hat{\\beta})\\)"
  },
  {
    "objectID": "regression.html#summary-of-slr",
    "href": "regression.html#summary-of-slr",
    "title": "Inference Models",
    "section": "SUMMARY OF SLR",
    "text": "SUMMARY OF SLR\nMain Formula\n\\(Y = \\alpha + \\beta X + \\epsilon\\)\nVariance & Slope Distribution (\\(\\beta\\)s)\n\\(\\hat{\\sigma^2} = \\frac{SSE}{n-2}\\)\n\\(SE(\\hat{\\beta}) = \\frac{\\sigma^2}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}}\\)\nCI for t-tests on \\(\\beta\\)\n\n\\(\\hat{\\beta} \\pm t_{\\alpha / 2, df = n-2} SE(\\hat{\\beta})\\)\nCoefficient of Determination\n\n\n\n\n\n\n\n\n\n\nSSE\nSSR\nSST\n\\(R^2\\)\n\n\n\nDescription\nMeasure of how much variation in \\(Y\\) is left unexplained by the model\nHow much variation in \\(Y\\) is explained by our model\nTotal amount of variation in observed \\(Y\\) values (what we would get if we used the mean of the data as our model)\nThe proportion of variance that can be explained by the model\n\n\nFormula\n\\(\\sum\\limits_{i=1}^{n} (y_i - \\hat{y_i})^2\\)\n\\(\\sum\\limits_{i=1}^n (\\hat{y_i} - \\bar{y})^2\\)\n\\(\\sum\\limits_{i=1}^n (y_i - \\bar{y})^2\\)\n\\(1 - \\frac{SSE}{SST}\\)\n\n\n\nThe closer to 1 \\(R^2\\) is, the better the model fits the data."
  },
  {
    "objectID": "regression.html#workflow-for-simple-linear-regression",
    "href": "regression.html#workflow-for-simple-linear-regression",
    "title": "Inference Models",
    "section": "Workflow for Simple Linear Regression",
    "text": "Workflow for Simple Linear Regression\n\nPlot the data as a scatter plot\n\n\nDoes linearity seem appropriate?\nCompute and overlay best-fit line\n\n\nConsider assumptions in SLR\n\n\nPlot a histogram of the residuals (are they normal?): WANT NORMAL\n\nPlot the residuals against x (are they changing?): WANT RANDOM\n\n\n\nInclude Plots"
  },
  {
    "objectID": "regression.html#simple-linear-regression-in-r",
    "href": "regression.html#simple-linear-regression-in-r",
    "title": "Inference Models",
    "section": "Simple Linear Regression in R",
    "text": "Simple Linear Regression in R\nThe following functions are paramount in implementing and interpreting simple linear regression in R:\n\nlm()\nsummary()\n\n\nLoad Library\n\n\nCode# import libraries\nlibrary(tidyverse)\n\n\n\nLoad Data (use txhousing from built in data)\n\n\nCodedf &lt;- txhousing\nhead(df)\n\n# A tibble: 6 × 9\n  city     year month sales   volume median listings inventory  date\n  &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Abilene  2000     1    72  5380000  71400      701       6.3 2000 \n2 Abilene  2000     2    98  6505000  58700      746       6.6 2000.\n3 Abilene  2000     3   130  9285000  58100      784       6.8 2000.\n4 Abilene  2000     4    98  9730000  68600      785       6.9 2000.\n5 Abilene  2000     5   141 10590000  67300      794       6.8 2000.\n6 Abilene  2000     6   156 13910000  66900      780       6.6 2000.\n\n\n\nCreate a simple linear regression model to see if we can explain and predict volume from listings\n\n\nCodeslr &lt;- lm(volume ~ listings, data = df)\nsummary(slr)\n\n\nCall:\nlm(formula = volume ~ listings, data = df)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-704067392  -27385680  -12084812    -951885 1686521981 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1501601.1  1750052.1  -0.858    0.391    \nlistings       36990.0      258.1 143.319   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 130500000 on 7174 degrees of freedom\n  (1426 observations deleted due to missingness)\nMultiple R-squared:  0.7411,    Adjusted R-squared:  0.7411 \nF-statistic: 2.054e+04 on 1 and 7174 DF,  p-value: &lt; 2.2e-16\n\n\nFrom this summary, we can see that listings is a statistically significant factor (low p-value) and that the \\(R^2\\) for the model is somewhat acceptable at \\(0.7411\\). Although the intercept being negative does raise some concern, this would mean that for a time period with \\(0\\) listings, we would get a negative volume. The model also says that for each new listing, we can expect the volume to increase by \\(36990\\)."
  },
  {
    "objectID": "regression.html#matrix-algebra-applications-solutions",
    "href": "regression.html#matrix-algebra-applications-solutions",
    "title": "Inference Models",
    "section": "Matrix Algebra Applications & Solutions",
    "text": "Matrix Algebra Applications & Solutions\nIf we have a perfectly square \\(X\\) matrix, then we can solve for \\(\\beta\\) via:\n\\(Y = X \\beta \\rightarrow X^{-1} Y = \\beta\\).\nHowever, we can almost guarantee that \\(X\\) will not be square. Look at this following linear algebra application:\n\nGiven a matrix \\(A\\) of any dimensions, both \\(A^TA\\) and \\(AA^T\\) will result in square matrices.\n\nTherefore, given \\(Y = X \\beta\\), we can solve via the following:\n\\((X^TX)^{-1}X^TY = \\beta\\)"
  },
  {
    "objectID": "regression.html#interpreting-mlr",
    "href": "regression.html#interpreting-mlr",
    "title": "Inference Models",
    "section": "Interpreting MLR",
    "text": "Interpreting MLR\nGiven \\(y_i = \\beta_0 + B_1 x_{i, 1} + \\dots + B_p x_{i, p} + \\epsilon_i\\), parameter \\(\\beta_k\\) is the expected change in the response associated with a unit change in the value of feature \\(x_k\\) while all of the other features are held fixed."
  },
  {
    "objectID": "regression.html#quantifying-goodness-of-fit",
    "href": "regression.html#quantifying-goodness-of-fit",
    "title": "Inference Models",
    "section": "Quantifying Goodness of Fit",
    "text": "Quantifying Goodness of Fit\n\n\\(R^2\\) vs. \\(R_a^2\\)\n\nLike in SLR, we can also calculate measures like SSE, SST, and \\(R^2\\) for MLR.\n\n\\(SSE = \\sum\\limits_{i=1}^n (y_i - \\hat{y_i})^2 = \\sum\\limits_{i=1}^n (y_i - (\\beta_0 + B_1 x_{i, 1} + \\dots + B_p x_{i, p})})^2\\)\n\\(SST = \\sum\\limits_{i=1}^n (y_i - \\bar{y})^2\\)\n\\(R^2 = 1 - \\frac{SSE}{SST}\\)\n\nAlthough we can calculate \\(R^2\\) for an MLR model, we run into the issue of multiple comparisons. The Adjusted \\(R^2\\), \\(R_a^2\\), is a better indicator of goodness of fit for MLR models. The adjusted version penalizes for having too many features that are not reducing SSE.\n\\(R_a^2 = 1 - \\frac{SSE/(n-p-1)}{SST/(n-1)}\\)\nCovariance & Correlation\nWe can discover relationships among features by performing a correlation analysis. If the value of one feature changes, how will this affect the other features.\nTraditionally, these measurements are calculated via:\n\nCovariance: \\(Cov(X, Y) = E[(X - E[X])(Y -E[Y])]\\)\n\nCorrelation (Pearson’s): \\(\\rho(X, Y) = \\frac{Cov(X, Y)}{\\sqrt{Var(X) Var(Y)}}\\)\n\n\nHowever, in an MLR, we estimate these relationships using formulas analogous to the sample variance.\n\nSample Covariance: \\(S^2_{XY} = \\frac{1}{n-1} \\sum\\limits_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\)\n\nSample Correlation \\(\\hat{\\rho}(X &lt; Y) = \\frac{S^2_{XY}}{\\sqrt{S^2_{X}S^2_{Y}}}\\)\n\n\nwhere \\(S^2_{X}\\) and \\(S^2_{Y}\\) are the variances for \\(X\\) and \\(Y\\), respectively."
  },
  {
    "objectID": "regression.html#individual-t-tests",
    "href": "regression.html#individual-t-tests",
    "title": "Inference Models",
    "section": "Individual t-tests",
    "text": "Individual t-tests"
  },
  {
    "objectID": "confidence_intervals.html",
    "href": "confidence_intervals.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "This page features confidence intervals’ definition, interpretation, and programming."
  },
  {
    "objectID": "confidence_intervals.html#ci-for-means",
    "href": "confidence_intervals.html#ci-for-means",
    "title": "Confidence Intervals",
    "section": "CI for Means",
    "text": "CI for Means\nThe following example is for a \\(100 * (1 - \\alpha) \\%\\) confidence interval for mean \\(\\mu\\) when the value of \\(\\sigma\\) is known:\n\\([\\bar{x} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\bar{x} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}]\\)"
  },
  {
    "objectID": "confidence_intervals.html#ci-for-difference-in-means",
    "href": "confidence_intervals.html#ci-for-difference-in-means",
    "title": "Confidence Intervals",
    "section": "CI for Difference in Means",
    "text": "CI for Difference in Means\n\\(\\bar{x} - \\bar{y} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\sigma_{x}^{2}}{n_x} + \\frac{\\sigma_{y}^{2}}{n_y}}\\)"
  },
  {
    "objectID": "confidence_intervals.html#ci-for-proportions-and-difference-in-proportions",
    "href": "confidence_intervals.html#ci-for-proportions-and-difference-in-proportions",
    "title": "Confidence Intervals",
    "section": "CI for Proportions and Difference in Proportions",
    "text": "CI for Proportions and Difference in Proportions"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\nCode1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Statistics",
    "section": "",
    "text": "A website dedicated to knowledge obtained for the statistics used in data science."
  },
  {
    "objectID": "review.html",
    "href": "review.html",
    "title": "Basic Review",
    "section": "",
    "text": "This page features review of basic statistical concepts and the linear algebra necessary for advanced statistics."
  },
  {
    "objectID": "review.html#discrete-random-variables",
    "href": "review.html#discrete-random-variables",
    "title": "Basic Review",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\nProbability Mass Function (pmf)\n\\(p(x) = P(X=x)\\)\n\n\\(p(x) \\geq 0\\)\n\\(\\sum_{x} p(x) = 1\\)\nCumulative Distribution Function (cdf)\n\\(F(x) = P(X \\leq x) = \\sum_{k \\leq x} p(k)\\)\n\n\\(0 \\leq F(x) \\leq 1\\)\nIf \\(a \\leq b \\rightarrow F(a) \\leq F(b)\\)"
  },
  {
    "objectID": "review.html#continuous-random-variables",
    "href": "review.html#continuous-random-variables",
    "title": "Basic Review",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\nProbability Density Function (pdf)\nIf X is a continuous random variable, then \\(f(x)\\) is a pdf if\n\n\\(P(a \\leq X \\leq b) = \\int_a^b f(x)dx\\)\n\\(f(x) \\geq 0\\)\n\\(\\int_{-\\infty}^\\infty f(x)dx = 1\\)\nCumulative Distribution Function (cdf)\nProperties of the CDF for a continuous RV X:\n\n\\(P(X \\leq x) = F(x)\\)\n\\(F(x) \\geq 0\\)\n\\(\\lim_{x \\rightarrow \\infty} F(x) = 1\\)\n\\(\\lim_{x \\rightarrow -\\infty} F(x) = 0\\)\nPDF to CDF, and Back Again\nBy the fundamental theorem of calculus, if \\(f(x)\\) is the pdf of a continuous variable X, and \\(F(x)\\) is the cdf the continuous variable X, then:\n\n\\(\\frac{d}{dx}F(x) = f(x)\\)\n\\(F(x) = \\int_{-\\infty}^x f(t)dt\\)"
  },
  {
    "objectID": "review.html#conditional-probability",
    "href": "review.html#conditional-probability",
    "title": "Basic Review",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\\(P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\\)\n\\(\\rightarrow P(B \\mid A) = \\frac{P(A \\mid B)P(B)}{P(A)}\\)\nIn the reference to the above formulas, the following definitions apply:\n\n\n\\(P(B \\mid A)\\): Posterior Probability\n\n\\(P(B)\\): Prior Probability\n\n\\(P(A)\\): Evidence\n\nNote that when dealing with more than two events, due to the law of total probability, the following is true:\n\\(P(A) = \\sum_{i = 1}^{n} P(A \\mid B_i)P(B_i)\\)\nIn other words, this applies:\n\\(P(B \\mid A) = \\frac{P(A \\mid B)P(B)}{P(A)} = \\frac{P(A \\mid B_j)P(B_j)}{\\sum_{i = 1}^{n} P(A \\mid B_i)P(B_i)}\\)"
  },
  {
    "objectID": "review.html#independence",
    "href": "review.html#independence",
    "title": "Basic Review",
    "section": "Independence",
    "text": "Independence\nGiven the events \\(A\\) and \\(B\\), the events are considered independent if\n\\(P(A \\cap B) = P(A)P(B)\\).\nThis has implications to the conditional probability definitions above."
  },
  {
    "objectID": "review.html#expected-value",
    "href": "review.html#expected-value",
    "title": "Basic Review",
    "section": "Expected Value",
    "text": "Expected Value\nExpected value, also known as expectation, mean, or average has slightly different methods of being calculated depending on if the random variable is discrete or continuous.\nDiscrete\n\\(E[X] = \\sum_{x}xp(x)\\)\n\\(E[g(X)] = \\sum_{x}g(x)p(x)\\)\nContinuous\n\\(E[X] = \\int_{-\\infty}^{\\infty} xf(x)dx\\)\n\\(E[g(X)] = \\int_{-\\infty}^{\\infty} g(x)f(x)dx\\)\nExpectation is a Linear Function\n\\(E[aX + b] = aE[X] + b\\)"
  },
  {
    "objectID": "review.html#variance-and-standard-deviation",
    "href": "review.html#variance-and-standard-deviation",
    "title": "Basic Review",
    "section": "Variance and Standard Deviation",
    "text": "Variance and Standard Deviation\nVariance, which essentially describes the spread of the data, particularly in reference to the mean, also has slightly different methods of being calculated depending on if the random variable is discrete or continuous.\nDiscrete\n\\(Var(X) = E[(X - E[X])]^2 = E[X^2] - (E[X])^2\\)\nContinuous\nNote: Notice from the formula above, all we really need to find are two different instances of expected value, namely\n\\(E[X^2]\\) and \\(E[X]\\)\nStandard Deviation\n\\(SD(X) = \\sqrt{Var(X)}\\)\nIndependence in Variance\nIf \\(X\\) and \\(Y\\) are independent, then\n\\(Var(X + Y) = Var(X) + Var(Y)\\)"
  },
  {
    "objectID": "review.html#percentiles-quantiles",
    "href": "review.html#percentiles-quantiles",
    "title": "Basic Review",
    "section": "Percentiles & Quantiles",
    "text": "Percentiles & Quantiles\nPercentiles\nFor a random variable \\(X\\), \\(x_p\\) is the \\(p^{th}\\) percentile of \\(X\\) if\n\\(P(X \\leq x_p) = p\\)\nFor example, the median is the \\(50^{th}\\) percentile.\nQuantiles\nWe often say percentile when given a percentage. The quantile is essentially just the decimal version of the percentile.\nFor example, the median is the \\(0.5\\) quantile.\nThe quantile function is the inverse of the CDF:\n\\(Q(q) = x\\) whenever \\(P(X \\leq x) = q\\)."
  },
  {
    "objectID": "review.html#symmetry",
    "href": "review.html#symmetry",
    "title": "Basic Review",
    "section": "Symmetry",
    "text": "Symmetry\nThe normal distribution is symmetric around its mean \\(\\mu\\). Hence, if we denote the normal distribution by \\(f_X(x)\\), then for any \\(c\\), \\(f_X(\\mu + c) = f_X(\\mu - c)\\).\nThis idea can be generalized for any distribution function. Suppose a distribution function, \\(f(x)\\), is symmetric around a point \\(x_0\\). Then for any \\(c\\), \\(f(x_0 + c) = f(x_0 - c)\\).\nIt can also be shown that if \\(X\\) is a continuous random variable whose ditribution is symmetric about \\(x_0\\), let’s use \\(x_0 = 0\\) for simplicity, then \\(E[2^{2k+1}] = 0\\), for any postive integer \\(k\\). That is, the expected value of any odd power of \\(X\\) is \\(0\\)."
  },
  {
    "objectID": "review.html#rule",
    "href": "review.html#rule",
    "title": "Basic Review",
    "section": "68-95-99.7 Rule",
    "text": "68-95-99.7 Rule\nGiven the random variable, \\(X \\sim N(0, 1)\\), we’ll examine the outcomes of when \\(X\\) is within 1, 2, and 3 standard deviations of the mean. We’ll build off the ideas of symmetry with symmetric ranges.\n\nSingle Standard Deviation From the Range\n\n\\(P(|x - \\mu| \\leq \\sigma)\\)\n\\(= 2P(x - \\mu \\leq \\sigma)\\) (just to show symmetry)\nOR\n\\(= P(-\\sigma \\leq x - \\mu \\leq \\sigma) = P(-1 \\leq \\frac{x-\\mu}{\\sigma} \\leq 1)\\)\n\\(= P(-1 \\leq Z \\leq 1)\\)\n\\(\\Phi(1) - \\Phi(-1)\\)\nNOTE: This does not depend on \\(\\mu\\) or \\(\\sigma\\).\nThe exact result of \\(P(|x - \\mu| \\leq \\sigma) \\approx 0.68\\)\n\nDouble Standard Deviation From the Range\n\n\\(P(|x - \\mu| \\leq 2\\sigma) \\approx 0.95\\)\n\nTriple Standard Deviation From the Range\n\n\\(P(|x - \\mu| \\leq 3\\sigma) \\approx 0.997\\)\nThis means that 99.7% of values for any normal distributions are within 3 standard deviations of the mean.\nThis rule is ONLY for normal distributions, but we can extend to other distributions."
  },
  {
    "objectID": "review.html#markovs-inequality",
    "href": "review.html#markovs-inequality",
    "title": "Basic Review",
    "section": "Markov’s Inequality",
    "text": "Markov’s Inequality\nGiven a random variable \\(X\\) such that \\(x \\geq 0\\) (this can be any distribution), then\n\\(P(X \\geq a) \\leq \\frac{E[X]}{a}\\)"
  },
  {
    "objectID": "review.html#chebyshevs-inequality",
    "href": "review.html#chebyshevs-inequality",
    "title": "Basic Review",
    "section": "Chebyshev’s Inequality",
    "text": "Chebyshev’s Inequality\nGiven a random variable \\(X\\) with finite variance, then\n\\(P(|X-\\mu|\\geq k\\sigma) \\leq \\frac{1}{k^2}\\)\nThis is saying that the probability of \\(X\\) being outside k standard deviations of mean is less than \\(\\frac{1}{k^2}.\\)\nNOTE: Different distributions (i.e. not normal distributions) are common in different domains. Markov’s and Chebyshev’s inequalities are great without knowing the exact distribution. In other words, they’re great for making general assumptions about general distributions."
  },
  {
    "objectID": "review.html#common-discrete-distributions",
    "href": "review.html#common-discrete-distributions",
    "title": "Basic Review",
    "section": "Common Discrete Distributions",
    "text": "Common Discrete Distributions\nBinomial Distribution\nIf a success has probability p, and there are n trials, what is the probability we have k successes:\n(Suppose a coin lands on heads with the probability p. If we flip the coin n times, what is the probability the coin lands on heads k times)\n\\(P(X=k) = \\binom{n}{k}p^k(1-p)^{n-k}\\)\n\n\\(E[X] = np\\)\n\\(Var(X) = np(1-p)\\)\nGeometric Distribution\nIf a success has probability p, what si the probability we k failures before the first success?\n\\(P(X = k) = p(1-p)^k\\)\n\n\\(E[X] = \\frac{1-p}{p}\\)\n\\(Var(X) = \\frac{1-p}{p^2}\\)\n\nDeriving E[X]\n\\(E[X] = \\sum_{k=0}^{\\infty} kp(1-p)^k\\)\n\\(= p(1-p)\\sum_{k=1}^{\\infty} kp(1-p)^{k-1}\\)\nNote that \\(kp(1-p)^{k-1}\\) is the definition of a derivative.\nSo, letting \\(q = 1-p\\), and using the fact that \\(\\sum \\frac{d}{dx}f(x) = \\frac{d}{dx}(\\sum f(x))\\),\n\\(E[X] = p(1-p)\\sum_{k=0}^{\\infty} (q^k)^{'} = p(1-p)(\\sum_{k=0}^{\\infty} q^k)^{'}\\)\nNote that \\(\\sum_{k=0}^{\\infty} q^k = \\frac{1}{1-q}\\) for \\(q \\leq 1\\)\n\\(= p(1-p)(\\frac{1}{1-q})^{'}\\)\n\\(= p(1-p)\\frac{1}{(1-q)^2}\\)\nSubstitute back \\(q = 1-p\\),\n\\(= \\frac{p(1-p)}{p^2} = \\frac{1-p}{p} = E[X]\\)\nDeriving Var(X)\n\\(Var(X) = E[X^2] - (E[X])^2\\)\nWe know \\(E[X]\\), so we only need to solve for \\(E[X^2]\\).\n\\(E[X^2] = \\sum_{k=0}^{\\infty} k^2p(1-p)^k = \\sum_{k=0}^{\\infty} k(k-1+1)p(1-p)^k\\)\n\\(= \\sum_{k=0}^{\\infty} k(k-1)p(1-p)^k + \\sum_{k=0}^{\\infty} kp(1-p)^k\\)\n\\(= p(1-p)^2 \\sum k(k-1)(1-p)^{k-2} + E[X]\\)\nNow just focusing on the left term, notice this is now the definition of a second derivative:\n\\(p(1-p)^2 \\sum ((1-p)^k)^{''}\\)\nPoisson Distribution\nSuppose we wanted to approximate the binomial distribution, \\(P(X=k) = \\binom{n}{k}p^k(1-p)^{n-k}\\), to find what it calculates on average for large numbers.\nSince \\(E[X] = np\\) (“on average”), let\n\\(\\lambda = np\\)\n\\(\\rightarrow p = \\frac{\\lambda}{n}\\)\nPlugging the \\(\\lambda\\) expression into the binomial distribution yields:\n\\(P(X=k) = \\binom{n}{k}(\\frac{\\lambda}{n})^k(1-(\\frac{\\lambda}{n}))^{n-k}\\)\nNow that we have an equation in terms of \\(\\lambda\\), what happens when we let \\(n \\rightarrow \\infty\\)?\n\\(\\binom{n}{k}(\\frac{\\lambda}{n})^k(1-(\\frac{\\lambda}{n}))^{n-k}\\)\n\\(= \\binom{n}{k}\\frac{(\\frac{\\lambda}{n})^k}{(1-\\frac{\\lambda}{n})^k}(1-\\frac{\\lambda}{n})^{n}\\)\nNote the following:\n\\(\\lim_{n \\rightarrow \\infty} (1-\\frac{\\lambda}{n})^{n} = e^{-\\lambda}\\)\nSo, just focusing on the left side of the equation:\n\\(\\binom{n}{k}\\frac{(\\frac{\\lambda}{n})^k}{(1-\\frac{\\lambda}{n})^k}\\)\n\\(= \\binom{n}{k}\\frac{\\lambda^k}{(n - \\lambda)^k}\\)\n\\(= \\frac{\\lambda^k}{k!}\\)\nTherefore,\n\\(\\lim_{n \\rightarrow \\infty} \\binom{n}{k}(\\frac{\\lambda}{n})^k(1-(\\frac{\\lambda}{n}))^{n-k} = \\frac{\\lambda^k}{k!} e^{-\\lambda}\\)\nTHEREFORE…\nFor large values of \\(n\\), the pmf for the binomial distribution is approximated by\n\\(\\frac{\\lambda^k}{k!} e^{-\\lambda}\\); where \\(\\lambda\\) is the average rate of success, and is known as the Poisson distribution. The Poisson distribution models the number of rare events in time or space.\nFor a more complete definition:\n\\(X \\sim Pois(\\lambda)\\) \\(P(X=k) = \\frac{\\lambda^k}{k!} e^{-\\lambda}\\) \\(E[X] = Var(X) = \\lambda\\)\nTo prove this is a probability mass function (pmf), we can use the Taylor’s Series:\n\\(\\sum_{k=0}^\\infty \\frac{\\lambda^k}{k!} = e^\\lambda\\)\n\\(\\sum_{k=0}^\\infty \\frac{\\lambda^k}{k!} e^{-\\lambda} = e^\\lambda \\sum_{k=0}^\\infty \\frac{\\lambda^k}{k!}\\)\n\\(= e^{-\\lambda}e^\\lambda = 1\\)\nRare: essentially means that two events cannot occur at once.\nUniform Distribution"
  },
  {
    "objectID": "review.html#common-continuous-distributions",
    "href": "review.html#common-continuous-distributions",
    "title": "Basic Review",
    "section": "Common Continuous Distributions",
    "text": "Common Continuous Distributions\nUniform Distribution\n\n\\(X \\sim Unif(a, b):\\)\n\n\\(f(x) = \\frac{a}{b-a}\\); \\(a \\leq x \\leq b\\)\n\ncdf: \\(P(X \\leq x) = F(x) = \\frac{x-a}{b-a}\\); \\(a \\leq x \\leq b\\)\n\n\n\\(E[X] = \\frac{a+b}{2}\\) middle of the interval\n\n\\(Var(X) = \\frac{(b-a)^2}{12}\\)\n\nCalculations:\ncdf:\n\\(F(x) = \\int_{-\\infty}^{x}f(t)dt = \\int_{-\\infty}^{x}f(t)dt + \\int_{a}^{x}f(t)dt\\) \\(= \\int_{a}^{x}\\frac{a}{b-a}dt = \\frac{x-a}{b-a}\\)\nE[X]:\n\\(E[X] = \\int_{-\\infty}^{\\infty} xf(x)dx\\) \\(= \\int_{a}^{b} x \\frac{1}{b-a} dx\\) \\(= \\frac{1}{2} x^2 \\frac{a}{b-a} \\mid_{a}^{b}\\)\n…simplifying…\n\\(= \\frac{a+b}{2}\\)\nVar(X):\n\\(Var(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2\\)\nWe only need to solve for \\(E[X^2]\\):\n\\(E[X^2] = \\int_{-\\infty}^{\\infty} x^2 \\frac{1}{b-a} dx\\) \\(= \\frac{1}{3} x^3 \\frac{a}{b-a} \\mid_{a}^{b}\\)\nPutting these together:\n\\(Var(X) = \\frac{(b-a)^2}{12}\\)\nNormal Distribution\nNormal Distribution: \\(\\sim N(\\mu, \\sigma)\\),\n\n\n\\(\\mu\\): expected value\n\nshifts the curve left or right\n\n\n\n\\(\\sigma\\): standard deviation (NOT variance)\n\nchanges width AND height\n\n\n\\(f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n\nStandard Normal Distribution:\nThe probability of the normal distribution, \\(P(a \\leq X \\leq B) = \\int\\limits_a^bf(x)dx\\), cannot be computed in closed form. only numerical integral can be used to find this. However, the Standard Normal Form provides normally quick computations through pre-computed values.\n\nThe Standard Normal Form: \\(Z \\sim N(0, 1)\\)\n\n\n\nClaim: \\(\\frac{X-\\mu}{\\sigma} \\sim N(0, 1)\\)\n\nLet \\(X\\) be a random variable, then \\(Z = \\frac{X-\\mu}{\\sigma}\\). We want to show that \\(E[Z] = 0\\) and \\(SD(Z) = 1\\)\nUsing the fact that expected value is linear:\n\\(E[\\frac{X-\\mu}{\\sigma}] = \\frac{E[X]-E[\\mu]}{E[\\sigma]} = \\frac{\\mu - \\mu}{E[\\sigma]} = 0\\)\nAside: \\(Var(aX) = a^2Var(X)\\)\n\\(Var(aX) = E[(aX)^2] - E[aX]^2\\)\n\\(= a^2E[X^2] - a^2E[X]^2 = a^2(E[X^2] - E[X]^2)\\)\n\\(= a^2Var(X)\\)\nUsing the above finding…\n\\(Var(Z) = Var(\\frac{X-\\mu}{\\sigma}) = \\frac{1}{\\sigma^2}Var(X-\\mu) = \\frac{1}{\\sigma^2}Var(X) = \\frac{\\sigma^2}{\\sigma^2} = 1\\)\n\\(Var(Z) = 1 \\rightarrow SD(Z) = \\sqrt{Var(Z)} = 1\\)\nWhat does the Z-score do?\nZ-score is defined as \\(Z = \\frac{x-\\mu}{\\sigma}\\), and measures how many standard deviations X is above (or below) the mean.\nThe Z-score calculates \\(P(Z \\leq x)\\), also commonly written as \\(\\Phi (X)\\), can be shifted to find other probabilities:\n\n\\(P(Z \\geq x) = 1 - P(Z \\leq x)\\)\n\\(P(a \\leq Z \\leq b) = P(Z \\leq b) - P(Z \\leq a)\\)\nExponential Distribution\n\nContinuous derivation of the Poisson Distribution\n\nRecall the Poisson distribution, which we used to model the number of rare events in time or space. The exponential distribution is the distribution of wait times in a Poisson process, i.e. how long until the next event.\n\n\n\\(F(X) = P(X \\leq x) = 1 - e^{-\\lambda x}\\), where \\(x&gt;0\\)\n\n\n\\(f(x) = \\lambda e^{-\\lambda x}\\), where \\(x&gt;0\\)\n\n\\(E[X] = \\frac{1}{\\lambda}\\)\n\\(Var(X) = \\frac{1}{\\lambda^2}\\)\n\n\\(X \\sim Exp(\\lambda)\\), where \\(\\lambda\\): the rate of occurrence of events\nMemoryless\n\nThe exponential distribution has the unique property of being memoryless, which can be illustrated with:\n\\(P(X &gt; t + s | X &gt; t) = P(X &gt; s)\\)\nTo put this result into words, it is saying “After waiting \\(t\\), the probability of an event happening in another \\(s\\) has the same probability of it happening in \\(s\\), if from \\(time = 0\\).”\nThis can be shown mathematically:\n\\(P(X &gt; t + s | X &gt; t) = \\frac{P(X &gt; t + s, X &gt; t)}{P(X &gt; t)}\\)\n\\(= \\frac{P(X &gt; t + s)}{P(X &gt; t)}\\)\n\\(= \\frac{1 - F(t+s)}{1 - F(t)}\\)\n\\(= \\frac{e^{-\\lambda(t+s)}}{e^{-\\lambda t}}\\)\n\\(= e^{-\\lambda s} = P(X &gt; s)\\)\n\nPoisson is “how many events in a time period”, while exponential is “what is the wait time between events”.\n\nAnother property of memorylessness is that:\n\\(E[X | X \\geq s] = s + E[X]\\)"
  },
  {
    "objectID": "review.html#central-limit-theorem",
    "href": "review.html#central-limit-theorem",
    "title": "Basic Review",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe idea behind the central limit theorem begins with the law of large numbers.\n\nLaw of Large Numbers\n\nAs the number of samples increases, the sample mean approaches the true mean:\nAs \\(n \\rightarrow \\infty\\), \\(\\bar{X_n} = \\frac{\\sum\\limits_x x_i}{n} \\rightarrow \\mu\\).\nAn application of Chebyshev’s Inequality provides the Weak LLN:\n\\(\\lim\\limits_{n \\rightarrow \\infty} P(|\\bar{X_n} - \\mu| &lt; \\epsilon) = 1\\)\n\nWeak Law of Large Numbers (WLLN)\n\nSpecifically, given\n\\(P(|\\bar{X_n} - \\mu| &gt; c) \\leq \\frac{\\sigma^2}{n * c^2}\\),\nThen taking the limit as \\(n \\rightarrow \\infty\\),\n\\(\\lim\\limits_{n \\to +\\infty} P(|\\bar{X}_n - \\mu| &gt; c) \\leq \\lim\\limits_{n \\to +\\infty} \\frac{\\sigma^2}{nc^2} = 0\\)\nDue to the axiom of probability, for any event \\(A\\), \\(P(A) \\geq 0\\), then we know that\n\\(0 \\leq (|\\bar{X}_n - \\mu| &gt; c) \\leq 0\\), then it follows that\n\\(\\lim\\limits_{n \\to +\\infty} P(|\\bar{X}_n - \\mu| &gt; c) = 0\\).\nIn conclusion, the WLLN says no matter how small we pick \\(c\\) to be, the probability of the sample mean being further away from the mean than that small number \\(c\\) goes to zero. Or, no matter how small we pick \\(c\\) to be, the probability of sample mean being with \\(c\\) of the true mean eventually approaches 1.\n\nThe Central Limit Theorem\n\nThis leads us to the Central Limit Theorem (CLT):\nAs \\(n \\rightarrow \\infty\\) for \\(\\bar{X} \\sim N(\\mu, \\frac{\\sigma}{\\sqrt{n}})\\),\nwhich holds regardless of what the underlying population distribution is.\nIn other words, the CLT tells us that the as the sample size \\(n\\) increases, the sample mean of \\(X\\) is close to normally distributed with expected value \\(\\mu\\) and standard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)."
  },
  {
    "objectID": "review.html#joint-variable-distributions",
    "href": "review.html#joint-variable-distributions",
    "title": "Basic Review",
    "section": "Joint Variable Distributions",
    "text": "Joint Variable Distributions\nGeneral Information\nCumulative Density Function: \\(F_{X,Y}(x, y) = P(X \\leq x, Y \\leq y)\\)\nDiscrete PMF: \\(p(x, y) = P(X=x, Y=y)\\)\nContinuous PDF: \\(f(x, y) = \\frac{\\partial^2 F(x, y)}{\\partial x \\partial y}\\)\nIndependence can be modeled via CDF, PMF, and PDF. If the following hold, the joint distributions are independent:\n\nCDF: \\(F(x, y) = F(x)F(y)\\)\n\npmf: \\(p(x, y) = p(x)p(y)\\)\n\npdf: \\(f(x, y) = f(x)f(y)\\)\n\nProperties of the Joint PMF\n\n\\(p(x, y) \\geq 0\\)\n\\(\\sum\\limits_x \\sum\\limits_y p(x, y) = 1\\)\n\nMarginals build from the idea of the law of total probability.\n\nmarginal pmf for \\(X\\): \\(p_X(a) = \\sum\\limits_y p(a, y)\\)\n\nmarginal pmf for \\(Y\\): \\(p_Y(b) = \\sum\\limits_x p(x, b)\\)\n\n\nConditional PMF: \\(p_{X|Y}(x|y) = P(X=x|Y=y) = \\frac{P(X=x, Y=y)}{P(Y=y)}\\)\nProperties of the joint PDF\n\n\\(f(x, y) \\geq 0\\)\n\\(\\int\\limits_x \\int\\limits_y f(x, y) = 1\\)\n\n\nGeometrically, the probability can be thought of as volume under a surface defined by the pdf.\n\nMarginals build from the idea of the law of total probability.\n\nmarginal pdf for \\(X\\): \\(f_X(a) = \\int\\limits_y f(a, y)\\)\n\nmarginal pdf for \\(Y\\): \\(f_Y(b) = \\int\\limits_y p(x, b)\\)\n\n\nConditional PDF: \\(f_{X|Y}(x|y) = P(X \\leq x|Y \\leq y) = \\frac{P(X \\leq x, Y \\leq y)}{P(Y \\leq y)}\\)"
  },
  {
    "objectID": "review.html#covariance-correlation",
    "href": "review.html#covariance-correlation",
    "title": "Basic Review",
    "section": "Covariance & Correlation",
    "text": "Covariance & Correlation\nThe exact definition of covariance for discrete and continuous:\n\nDiscrete: \\(Cov(X, Y) = \\sum\\limits_x \\sum\\limits_y (x - \\mu_x)(y - \\mu_y) p(x, y)\\)\n\nContinuous: \\(Cov(X, Y) = \\int\\limits_x \\int\\limits_y (x - \\mu_x)(y - \\mu_y) f(x, y)dxdy\\)\n\n\nA better, generic way to calculate covariance:\n\\(Cov(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]\\)\nProof:\n\\(Cov(X, Y) = E[(X - E[X])(Y - E[Y])]\\)\n\\(= E[XY + XE[Y] - YE[X] + E[X]E[Y]]\\)\n\\(= E[XY] + E[X]E[Y] - E[Y]E[X] + E[X]E[Y]\\)\n\\(= E[XY] - E[X]E[Y]\\)\nIf \\(X\\) and \\(Y\\) are independent, covariance is 0\nCorrelation:\n\\(Corr(X, Y) = \\frac{Cov(X, Y)}{SD(X)SD(Y)}\\)\nIf \\(X\\) and \\(Y\\) are independent, correlation is 0"
  },
  {
    "objectID": "review.html#distributions",
    "href": "review.html#distributions",
    "title": "Basic Review",
    "section": "Distributions",
    "text": "Distributions\nTo showcase the distribution formulas in R, we’ll use the example of the normal function. However, most common distributions share the same blueprint as the functions we’ll call.\nWe can use the normal distribution in R via the following:\n\n\ndnorm: Returns a value from the distribution provided a location, mean, and standard deviation. Calling a specific value from a distribution has more meaning and applications within discrete distributions.\n\npnorm: Returns a value from the cumulative distribution function (cdf) provided a location, mean, and standard deviation. In other words, returns the area to the left of the given value from the normal cdf. Optional parameter of lower.tail=FALSE to get the area to the right of the given value.\n\nqnorm: Returns the value of the \\(p^{th}\\) quantile (Z-score) provided a quantile, mean, and standard deviation.\n\nrnorm: Returns a vector of normally distributed random variables provided size of desired vector, mean, and standard deviation.\n\n\n\n\n\n\n\nNote\n\n\n\nImport Libraries\n\n\n\nCode# import libraries\nlibrary(tidyverse)\n\n\n\n\n\n\n\n\nNote\n\n\n\ndnorm: value of the standard normal distribution pdf at x=0.\n\n\n\nCode# dnorm: value of the standard normal distribution pdf at x=0\ndnorm(x = 0, mean = 0, sd = 1)\n\n[1] 0.3989423\n\n\n\n\n\n\n\n\nNote\n\n\n\npnorm: amount of the standard normal distribution pdf to the left of x=0.\n\n\n\nCode# pnorm: amount of the standard normal distribution pdf to the left of x=0\npnorm(q = 0, mean = 0, sd = 1)\n\n[1] 0.5\n\n\n\n\n\n\n\n\nNote\n\n\n\nqnorm: find the 0.50 quantile of the standard normal distribution.\n\n\n\nCode# qnorm: find the 0.50 quantile of the standard normal distribution\nqnorm(p = 0.50, mean = 0, sd = 1)\n\n[1] 0\n\n\n\n\n\n\n\n\nNote\n\n\n\nrnorm: vector of 1000 random draws from the standard normal distribution.\n\n\n\nCode# rnorm: vector of 1000 random draws from the standard normal distribution\nnormal_vector &lt;- rnorm(n = 1000, mean = 0, sd = 1)\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe’ll illustrate rnorm by plotting the density distribution of the vector created.\n\n\n\nCodeggplot() + \n  geom_density(aes(normal_vector), fill = 'blue', alpha = 0.5) +\n  xlab('Randomly Drawn Values from the Standard Normal Distribution') +\n  ylab('Density')\n\n\n\n\n\n\n\nAs previously stated, many of the common distributions have these built in functionalities within R. For example, the exponential distribution has the functions dexp, pexp, qexp, and rexp. These follow a similar blueprint as to what is seen with the normal distribution functions above, but may contain slightly different nomenclature and parameters."
  },
  {
    "objectID": "review.html#helpful-functions",
    "href": "review.html#helpful-functions",
    "title": "Basic Review",
    "section": "Helpful Functions",
    "text": "Helpful Functions\nThe Apply Family\nA useful assortment of functions in R are available through the apply family:\n\napply\nlapply\nsapply\ntapply\nSample and Replicate\nThe sample and replicate functions are useful individually and when combined can be incredibly useful for statistical modeling.\nAdditionally useful in statistical modeling is the quantile function."
  }
]