[
  {
    "objectID": "statistical_hypotheses.html",
    "href": "statistical_hypotheses.html",
    "title": "Statistical Hypotheses",
    "section": "",
    "text": "This page features theories involving statistical hypotheses, including:"
  },
  {
    "objectID": "statistical_hypotheses.html#classic-jury-analogy",
    "href": "statistical_hypotheses.html#classic-jury-analogy",
    "title": "Statistical Hypotheses",
    "section": "Classic Jury Analogy",
    "text": "Classic Jury Analogy\nConsider a jury in a criminal trial. When a defendant is accused of a crime, the jury presumes that they are not guilty.\n\n\n\\(H_0\\): Not Guilty\n\n\\(H_A\\): Guilty\n\nThe jury is then presented with evidence. If the evidence seems implausible under the assumption of non-guilt, we might reject the null hypothesis of non-guilt, and claim that the defendant is (likely) guilty.\n\nIn the case of statistical hypothesis testing, we use data to find evidence, leading us to arrive at two possible conclusions:\n\n\nReject the null hypothesis, \\(H_0\\), in favor of the alternative hypothesis, \\(H_A\\)\n\nFail to reject the null hypothesis, \\(H_0\\)"
  },
  {
    "objectID": "statistical_hypotheses.html#setting-up-the-hypothesis-test",
    "href": "statistical_hypotheses.html#setting-up-the-hypothesis-test",
    "title": "Statistical Hypotheses",
    "section": "Setting Up the Hypothesis Test",
    "text": "Setting Up the Hypothesis Test\nThe null hypothesis will almost always be represented with equivalence. The alternative hypothesis will be then be represented with an inequality or shown to be not equivalent.\n\n\n\\(H_0\\): \\(\\theta = \\theta_0\\)\n\n\n\\(H_A\\): One of the Following:\n\n\\(\\theta &gt; \\theta_0\\)\n\\(\\theta &lt; \\theta_0\\)\n\\(\\theta \\neq \\theta_0\\)\n\n\n\nThe alternative hypothesis is the hypothesis for which we are seeking statistical evidence for."
  },
  {
    "objectID": "statistical_hypotheses.html#rejection-regions-for-alternative-hypotheses",
    "href": "statistical_hypotheses.html#rejection-regions-for-alternative-hypotheses",
    "title": "Statistical Hypotheses",
    "section": "Rejection Regions for Alternative Hypotheses",
    "text": "Rejection Regions for Alternative Hypotheses\nThe following example uses the \\(z\\) statistic, but the concept holds for other statistics and critical values.\n\n\n\n\n\n\nAlternative Hypothesis\nRejection Region for Level \\(\\alpha\\) Test\n\n\n\n\n\\(H_A\\): \\(\\theta &gt; \\theta_0\\)\n\n\\(z \\geq z_{\\alpha}\\)\n\n\n\n\\(H_A\\): \\(\\theta &lt; \\theta_0\\)\n\n\\(z \\leq z_{\\alpha}\\)\n\n\n\n\\(H_A\\): \\(\\theta \\neq \\theta_0\\)\n\n\n\\(z \\geq z_{\\alpha}\\) OR \\(z \\leq z_{\\alpha}\\)"
  },
  {
    "objectID": "statistical_hypotheses.html#type-i-error",
    "href": "statistical_hypotheses.html#type-i-error",
    "title": "Statistical Hypotheses",
    "section": "Type I Error",
    "text": "Type I Error\nA Type I error is rejecting the null hypothesis when it is actually true. This is actually representing by \\(\\alpha\\), where significance level is set at \\(100*(1-\\alpha)\\)."
  },
  {
    "objectID": "statistical_hypotheses.html#type-ii-error",
    "href": "statistical_hypotheses.html#type-ii-error",
    "title": "Statistical Hypotheses",
    "section": "Type II Error",
    "text": "Type II Error\nA Type II error is failing to reject the null hypothesis when it is actually false. This is normally considered worse than Type I errors."
  },
  {
    "objectID": "statistical_hypotheses.html#visualizing-errors",
    "href": "statistical_hypotheses.html#visualizing-errors",
    "title": "Statistical Hypotheses",
    "section": "Visualizing Errors",
    "text": "Visualizing Errors"
  },
  {
    "objectID": "statistical_hypotheses.html#power",
    "href": "statistical_hypotheses.html#power",
    "title": "Statistical Hypotheses",
    "section": "Power",
    "text": "Power\nConsidering power helps to ensure high quality hypothesis tests.Recall” - Type I errors are when we reject the null hypothesis when it is true - Type II errors are when we fail to reject the null hypothesis when the alternative hypothesis is true\nThe power of a hypothesis test is the probability of making the correct decision if the alternative hypothesis is true. In other words, it is the probability of rejecting the null hypothesis when the alternative hypothesis is true.\nIf \\(\\beta\\) is the probability of making a Type II error, then\n\\(Power = 1 - \\beta\\)\nIn general, we want to minimize \\(\\alpha\\) and maximize power."
  },
  {
    "objectID": "statistical_hypotheses.html#visualizing-power",
    "href": "statistical_hypotheses.html#visualizing-power",
    "title": "Statistical Hypotheses",
    "section": "Visualizing Power",
    "text": "Visualizing Power"
  },
  {
    "objectID": "statistical_hypotheses.html#the-bootstrap-process",
    "href": "statistical_hypotheses.html#the-bootstrap-process",
    "title": "Statistical Hypotheses",
    "section": "The Bootstrap Process",
    "text": "The Bootstrap Process\n\nTake re-samples (with replacement) of the same size as the original sample.\nCompute the desired statistic for each re-sample. These statistics form the distribution.\nIf we’re specifically targeting a \\((100-\\alpha)\\%\\) confidence interval, then find the \\(\\alpha^{th}\\) and \\((1-\\alpha)^{th}\\) percentiles. This will be the CI."
  },
  {
    "objectID": "statistical_hypotheses.html#the-randomization-process",
    "href": "statistical_hypotheses.html#the-randomization-process",
    "title": "Statistical Hypotheses",
    "section": "The Randomization Process",
    "text": "The Randomization Process\nImage we have boolean results from two separate groups. In other words, within each group, some have the results of yes and some have the results of no. Between the two groups, there are different proportions of yes values. We want to test if there is an explanatory variable causing the difference in proportions, or if it is due to inherent variability.\nIn the following process, let’s consider each result a card, and each card will have a label of yes or no.\nThe groups will likely not be the same size, let’s call the length of Group 1 \\(n_1\\) and the length of Group 2 \\(n_2\\).\nCreating the Distribution\n\nShuffle all the cards. Group displacement will occur (i.e. after shuffling there will only be a total count of yes and no, and sense of group will be removed).\nPut \\(n_1\\) cards into Group 1 and \\(n_2\\) cards into Group 2\nCompute the different in proportions between yes for each group\nRepeat this many times to get a distribution of the differences in proportions\nPerforming the Hypothesis Test\nThe hypothesis test will look something of the form (alternative depends on how the test is setup):\n\n\n\\(H_0\\): \\(p_1 = p_2\\) (i.e. the true proportions of the group are equivalent)\n\n\\(H_A\\): \\(p_1 &gt; p_2\\) (i.e. the true proportions of Group 1 is greater than Group 2, or different in some manner depending on how the test is constructed)\n\nWhen performing the hypothesis test, we want to compare the critical value on rejection region with the test statistic.\nIf the test statistic is at least as extreme as the critical value calculated from the distribution (i.e. test statistic lies within the rejection region), for a specified significance level, then we can reject the null hypothesis in favor of the alternative hypothesis. If not, we lack sufficient evidence to reject the null hypothesis."
  },
  {
    "objectID": "statistical_hypotheses.html#inference-on-mean-and-proportions",
    "href": "statistical_hypotheses.html#inference-on-mean-and-proportions",
    "title": "Statistical Hypotheses",
    "section": "Inference on Mean and Proportions",
    "text": "Inference on Mean and Proportions\n\n\nCategory\n\\(n \\geq 30\\)\n\\(n &lt; 30\\)\n\n\n\nNormal Data, Known \\(\\sigma\\)\n\nz-test\nz-test\n\n\nNormal Data, Unknown \\(\\sigma\\)\n\nz-test\nt-test\n\n\nNon-Normal Data, Known \\(\\sigma\\)\n\nz-test\nbootstrap\n\n\nNon-Normal Data, Unknown \\(\\sigma\\)\n\nz-test\nbootstrap"
  },
  {
    "objectID": "statistical_hypotheses.html#inference-on-variance",
    "href": "statistical_hypotheses.html#inference-on-variance",
    "title": "Statistical Hypotheses",
    "section": "Inference on Variance",
    "text": "Inference on Variance\n\n\nCategory\n\\(n \\geq 30\\)\n\\(n &lt; 30\\)\n\n\n\nNormal Data\nchi-squared\nchi-squared\n\n\nNon-Normal Data\nfurther research required\nbootstrap"
  },
  {
    "objectID": "statistical_hypotheses.html#inference-on-anything-else-median-skew-discrimination-rate-etc.",
    "href": "statistical_hypotheses.html#inference-on-anything-else-median-skew-discrimination-rate-etc.",
    "title": "Statistical Hypotheses",
    "section": "Inference on Anything Else (Median, Skew, Discrimination Rate, etc.)",
    "text": "Inference on Anything Else (Median, Skew, Discrimination Rate, etc.)\n\n\nCategory\n\\(n \\geq 30\\)\n\\(n &lt; 30\\)\n\n\n\nNormal Data\nbootstrap\nbootstrap\n\n\nNon-Normal Data\nbootstrap\nbootstrap"
  },
  {
    "objectID": "statistical_hypotheses.html#hypothesis-testing",
    "href": "statistical_hypotheses.html#hypothesis-testing",
    "title": "Statistical Hypotheses",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing"
  },
  {
    "objectID": "statistical_hypotheses.html#bootstrapping-1",
    "href": "statistical_hypotheses.html#bootstrapping-1",
    "title": "Statistical Hypotheses",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "statistical_hypotheses.html#hypothesis-testing-with-randomization-1",
    "href": "statistical_hypotheses.html#hypothesis-testing-with-randomization-1",
    "title": "Statistical Hypotheses",
    "section": "Hypothesis Testing with Randomization",
    "text": "Hypothesis Testing with Randomization"
  },
  {
    "objectID": "statistical_hypotheses.html#hypothesis-testing-1",
    "href": "statistical_hypotheses.html#hypothesis-testing-1",
    "title": "Statistical Hypotheses",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing"
  },
  {
    "objectID": "statistical_hypotheses.html#bootstrapping-2",
    "href": "statistical_hypotheses.html#bootstrapping-2",
    "title": "Statistical Hypotheses",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nLecture 5: replace, sample, apply, and quantile.\n\nCodelibrary(tidyverse)\n\n\n\nCode# 1000 bootstrap samples\nset.seed(303)\ndata &lt;- runif(n=50, min = 0, max = 10)\nbootstrap_data &lt;- replicate(n = 1000, expr = sample(data, size = length(data), replace = TRUE))\nboostrap_mean &lt;- apply(X = bootstrap_data, MARGIN = 2, FUN = mean)\n\nggplot() +\n  geom_density(aes(x=boostrap_mean))\n\n\n\n\n\n\n\n\nCode# what about the CI?\nquantile(boostrap_mean, c(0.025, 0.975))\n\n    2.5%    97.5% \n4.221890 5.638946"
  },
  {
    "objectID": "statistical_hypotheses.html#hypothesis-testing-with-randomization-2",
    "href": "statistical_hypotheses.html#hypothesis-testing-with-randomization-2",
    "title": "Statistical Hypotheses",
    "section": "Hypothesis Testing with Randomization",
    "text": "Hypothesis Testing with Randomization\nProportion Example\n\nCodeprop_treatment &lt;- 45/69\nprop_control &lt;- 30/34\nprop_difference &lt;- prop_control - prop_treatment\nprop_difference\n\n[1] 0.230179\n\n\n\nCode# create_data function\ncreate_data &lt;- function(control_total, control_death,\n                        treatment_total, treatment_death) {\n    total_deaths &lt;- control_death + treatment_death\n    total_subjects &lt;- control_total + treatment_total\n    non_survivors &lt;- rep(1, total_deaths)\n    survivors &lt;- rep(0, total_subjects - total_deaths)\n\n    return(c(non_survivors, survivors))\n}\n\n\n\nCode# shuffle function\nsimulation &lt;- function(control_total = 34, control_death = 30,\n                       treatment_total = 69, treatment_death = 45) {\n    index_data &lt;- create_data(control_total, control_death,\n                              treatment_total, treatment_death)\n    shuffled_data &lt;- sample(x = index_data, size = length(index_data),\n                            replace = FALSE)\n    control &lt;- shuffled_data[1:control_total]\n    treatment &lt;- shuffled_data[(control_total+1):length(shuffled_data)]\n    \n    # proportion of deaths in control group\n    control_p &lt;- sum(control) / length(control)\n    \n    # proportion of deaths in treatment group\n    treatment_p &lt;- sum(treatment) / length(treatment)\n    \n    # compute difference\n    difference_p &lt;- control_p - treatment_p\n    \n    return(difference_p)\n}\n\n\n\nCode# run simulation 1000 times\nrandomized_differences &lt;- replicate(1000, simulation())\n\n\n\nCodedifference_quantile &lt;- quantile(x = randomized_differences, probs = c(0.95))\ndifference_quantile\n\n    95% \n0.14237 \n\n\n\nCodeggplot() +\n    geom_histogram(aes(x = randomized_differences, y = after_stat(density),\n                       color = 'Differences'), bins = 12) +\n    annotate('rect', xmin = difference_quantile, xmax = 0.3,\n             ymin = 0, ymax = 8, alpha = 0.1, fill = 'blue') +\n    geom_vline(mapping = aes(color = 'Rejection', xintercept = difference_quantile),\n               linewidth = 2) +\n    geom_vline(mapping =\n               aes(color = 'Sample Proportion',\n                   xintercept = prop_difference), linewidth = 2) +\n    xlab('Differences') +\n    ylab('Density') +\n    ggtitle('Proportion Differences Histogram') +\n    scale_color_manual(breaks = c('Differences', 'Rejection',  'Sample Proportion'),\n                       values = c('black', 'blue', 'green'),\n                       name = 'Legend')"
  },
  {
    "objectID": "model_selection.html",
    "href": "model_selection.html",
    "title": "Model Selection",
    "section": "",
    "text": "F-tests and t-tests are decent for simple models. We know that \\(R^2\\) holds significance for simple linear regression, and that \\(R_a^2\\) holds significance for multiple linear regression. We can use these tests and metrics to compare different features in models across datasets. But, how do we use this for more complicated models? What about the case for a model with many predictors? If we were test different models manually for a dataset with 100 features, that’s \\(2^{100}\\) combinations!"
  },
  {
    "objectID": "model_selection.html#aic",
    "href": "model_selection.html#aic",
    "title": "Model Selection",
    "section": "AIC",
    "text": "AIC\n\\[AIC = 2(p+1) - 2\\log(\\frac{SSE}{n})\\]\n\\(AIC\\) estimates the relative amount of information that is lost by a given model in effort to minimize the information that’s lost.\nThe first term is an estimate of complexity with respect to the model, while the second term is an estimate of model fit. Normally thought of as a decent metric for prediction goals, though this is not a “hard and fast” rule."
  },
  {
    "objectID": "model_selection.html#bic",
    "href": "model_selection.html#bic",
    "title": "Model Selection",
    "section": "BIC",
    "text": "BIC\n\\[BIC = (p+1)log(n) - 2logL(\\hat{\\beta})\\]\n\\(BIC\\) is similar estimate to \\(AIC\\) with slightly different parameters, where\n\n\n\\(logL(\\hat{\\beta})\\) is the log likelihood function\n\nBIC is normally thought of as a decent metric for explanation goals, though this is not a “hard and fast” rule. For a very large amount of features, the penalty term for BIC is larger than AIC."
  },
  {
    "objectID": "model_selection.html#mean-squared-prediction-error",
    "href": "model_selection.html#mean-squared-prediction-error",
    "title": "Model Selection",
    "section": "Mean Squared Prediction Error",
    "text": "Mean Squared Prediction Error\n\\[MSPE = \\frac{1}{n}\\sum^n_{i=1} (y_i - \\hat{y_i})^2\\]\n\\(MSPE\\) quantifies the discrepancy between the predicted values and the observed value, and can help to evalute the performance of a model."
  },
  {
    "objectID": "model_selection.html#goals-and-motivation",
    "href": "model_selection.html#goals-and-motivation",
    "title": "Model Selection",
    "section": "Goals and Motivation",
    "text": "Goals and Motivation\nOur goal is to ultimately select a model \\(g\\) (parameterized by \\(\\beta\\)) that is close to the true model \\(f\\).\nKullback-Leiber distance:\n\\(D_{KL}(f, g) = \\int f(x) log(\\frac{f(x)}{g(x; \\beta)}) dx\\)\n\\(= \\int f(x) log(f(x))dx - \\int f(x) log(g(x; \\beta))dx\\)\nThe first term is constant with respect to \\(g\\), and it can be shown the second term can be estimated to \\(-log(L(\\hat{\\beta})) + (p+1) + c\\) (i.e. model complexity + model fit)\n\\(\\rightarrow AIC(g(x; \\hat{\\beta}))\\)\n\\(\\rightarrow BIC(g(x; \\hat{\\beta}))\\)"
  },
  {
    "objectID": "model_selection.html#procedure-for-using-the-methods",
    "href": "model_selection.html#procedure-for-using-the-methods",
    "title": "Model Selection",
    "section": "Procedure for Using the Methods",
    "text": "Procedure for Using the Methods\n\nMSPE requires testing and training split datasets, while it is just a recommendation for AIC, BIC, and \\(R^2_a\\)\n\n\nSplit data into training/test sets\nFit several competing models (training set)\n\n\nCan use backwards, forwards, or bi-directional algorithms to help pick the models to test (more on this later)\n\n\nCalculate metric(s)\nChoose the optimal model\n\n\nMinimize \\(AIC\\)\n\nMinimize \\(BIC\\)\n\nMinimize \\(MSPE\\)\n\nMaximize \\(R^2_a\\)"
  },
  {
    "objectID": "model_selection.html#stepwise-methods-update-and-regsubsets",
    "href": "model_selection.html#stepwise-methods-update-and-regsubsets",
    "title": "Model Selection",
    "section": "Stepwise Methods (UPDATE and REGSUBSETS)",
    "text": "Stepwise Methods (UPDATE and REGSUBSETS)\nWe can systematically implement functions for forwards and backwards selection using update(), but one method that can be directly implemented in R is through regsubsets().\nUpdate\n\nCreate SLR, Update To Add Variable, Update to Remove Variable\n\n\nCode# create model to predict WP from OBP\nmod &lt;- lm(data=df, WP~OBP)\nsummary(mod)\n\n\nCall:\nlm(formula = WP ~ OBP, data = df)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.232282 -0.044727  0.001874  0.044186  0.185660 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.24102    0.03837  -6.282 4.64e-10 ***\nOBP          2.26964    0.11745  19.324  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06187 on 1230 degrees of freedom\nMultiple R-squared:  0.2329,    Adjusted R-squared:  0.2323 \nF-statistic: 373.4 on 1 and 1230 DF,  p-value: &lt; 2.2e-16\n\nCode# update model to add SLG\nmod &lt;- update(mod, . ~ . + SLG)\nsummary(mod)\n\n\nCall:\nlm(formula = WP ~ OBP + SLG, data = df)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.229835 -0.044301  0.001687  0.044303  0.181604 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.22136    0.04123  -5.369 9.46e-08 ***\nOBP          2.07235    0.19188  10.800  &lt; 2e-16 ***\nSLG          0.11257    0.08659   1.300    0.194    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06185 on 1229 degrees of freedom\nMultiple R-squared:  0.2339,    Adjusted R-squared:  0.2327 \nF-statistic: 187.7 on 2 and 1229 DF,  p-value: &lt; 2.2e-16\n\nCode# update model to remove OBP\nmod &lt;- update(mod, . ~ . - OBP)\nsummary(mod)\n\n\nCall:\nlm(formula = WP ~ SLG, data = df)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.220219 -0.045113  0.002577  0.046060  0.175795 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.16101    0.02210   7.286 5.71e-13 ***\nSLG          0.85224    0.05542  15.377  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06469 on 1230 degrees of freedom\nMultiple R-squared:  0.1612,    Adjusted R-squared:  0.1606 \nF-statistic: 236.4 on 1 and 1230 DF,  p-value: &lt; 2.2e-16\n\n\nRegsubsets\n\nCodereg_lm &lt;- regsubsets(data=df, WP ~ .)\nrs &lt;- summary(reg_lm)\nrs$which\n\n  (Intercept)   OBP   SLG    BA   RD Playoffs Champion\n1        TRUE FALSE FALSE FALSE TRUE    FALSE    FALSE\n2        TRUE FALSE FALSE FALSE TRUE     TRUE    FALSE\n3        TRUE FALSE FALSE FALSE TRUE     TRUE     TRUE\n4        TRUE FALSE FALSE  TRUE TRUE     TRUE     TRUE\n5        TRUE  TRUE FALSE  TRUE TRUE     TRUE     TRUE\n6        TRUE  TRUE  TRUE  TRUE TRUE     TRUE     TRUE\n\n\n\nEach row represents a progressive combination of features, representing the best model of different sizes, with each row adding a new feature. The best models are determined by lowest SSE, or sum squared error."
  },
  {
    "objectID": "model_selection.html#custom-function",
    "href": "model_selection.html#custom-function",
    "title": "Model Selection",
    "section": "Custom Function",
    "text": "Custom Function\n\nCustom Function for MSPE\n\n\nCode# function for returning mspe form a regsubset output\nmspe_loop &lt;- function(train_set, test_set, regsubset_summary, response) {\n    # observed values from test set\n    obs &lt;- test_set[[response]]\n    # initialize mspe tracking list\n    mspe_results &lt;- c()\n    loop_size &lt;- dim(regsubset_summary$which)[1]\n    for (model in 1:loop_size) {\n        # create model\n        true_cols &lt;- names(which(regsubset_summary$which[model,]))\n        # remove intercept feature\n        true_cols &lt;- true_cols[true_cols != '(Intercept)']\n        formula &lt;- paste(unlist(true_cols), collapse = '+')\n        formula &lt;- paste('~', formula, '')\n        formula &lt;- paste(response, formula, '')\n        lmod &lt;- lm(data = train_set, as.formula(formula))\n        # calculate MSPE\n        preds &lt;- lmod %&gt;% predict(test_set)\n        mspe &lt;- mean((obs - preds)^2)\n        mspe_results &lt;- c(mspe_results, mspe)\n    }\n    return(mspe_results)\n}\n\n\n\nCustom Function for Creating a Long Dataframe\n\n\nCode# function for making dataframe long with respect to response variable\ncreate_long_df &lt;- function(df, response) {\n    df_long &lt;- pivot_longer(df,\n                            cols = names(df %&gt;% select(-all_of(response))),\n                            names_to = 'feature_names',\n                            values_to = 'feature_values')\n    return(df_long)\n}\n\n\n\nCustom Function for Plotting Facets from a Long Dataframe\n\n\nCode# create function for faceted plots\nplot_long_facets &lt;- function(df_long, subset_type, response) {\n    title_text &lt;- paste(subset_type, response)\n    ggplot(df_long, aes(x=feature_values, y=!!sym(response))) +\n        geom_point() +\n        facet_wrap(~ feature_names, scales = 'free_x') +\n        ggtitle(title_text) +\n        xlab('Feature')\n}\n\n\n\nFunction for Full Model MLR Plotting\n\n\nCode# create function mlr the plotting\nplot_mlr &lt;- function(full_set, train_set, test_set, response, subset_type) {\n    # faceted plots\n    # 1) build long dataframe with respect to variabe\n    # 2) create plots\n    df_long &lt;- create_long_df(df=full_set, response=response)\n    faceted_plots &lt;- plot_long_facets(df_long=df_long,\n                                      subset_type=subset_type,\n                                      response=response)\n    print(faceted_plots)\n    \n    # regsubsets builds models with forward selection\n    formula &lt;- paste(response, '~.', '')\n    reg_lm &lt;- regsubsets(data=train_set, as.formula(formula))\n    rs &lt;- summary(reg_lm)\n    print(rs$which)\n    \n    # compute dimensions from dataset\n    n &lt;- dim(train_set)[1]\n    m &lt;- dim(train_set)[2]\n    x &lt;- 1:(m-1)\n\n    # compute metrics\n    AIC &lt;- 2*(2:m) + n*log(rs$rss/n)\n    BIC &lt;- log(n)*(2:m) + n*log(rs$rss/n)\n    R2Adj &lt;- rs$adjr2\n    mspe &lt;- mspe_loop(train_set=train_set,\n                      test_set=test_set,\n                      regsubset_summary=rs,\n                      response=response)\n\n    # turn metrics into dataframes\n    AIC_df &lt;- data.frame(AIC = AIC)\n    BIC_df &lt;- data.frame(BIC = BIC)\n    R2Adj_df &lt;- data.frame(R2Adj = R2Adj)\n    mspe_df &lt;- data.frame(MSPE = mspe)\n    \n    # AIC Plot\n    AIC_plot &lt;- ggplot(data = AIC_df, aes(x=x, y=AIC)) +\n        geom_line() +\n        geom_point(aes(x=which.min(AIC), y=min(AIC)), color='red', size=3) +\n        xlab('Model')\n\n    # BIC Plot\n    BIC_plot &lt;- ggplot(data = BIC_df, aes(x=x, y=BIC)) +\n        geom_line() +\n        geom_point(aes(x=which.min(BIC), y=min(BIC)), color='red', size=3) +\n        xlab('Model')\n\n    # Adjusted R2 Plot\n    R2Adj_plot &lt;- ggplot(data = R2Adj_df, aes(x=x, y=R2Adj)) +\n        geom_line() +\n        geom_point(aes(x=which.max(R2Adj), y=max(R2Adj)), color='red', size=3) +\n        xlab('Model')\n\n    # MSPE Plot\n    MSPE_plot &lt;- ggplot(data = mspe_df, aes(x=x, y=MSPE)) +\n        geom_line() +\n        geom_point(aes(x=which.min(mspe), y=min(mspe)), color='red', size=3) +\n        xlab('Model')\n    \n    # combine plots\n    combined_plot &lt;- ggarrange(AIC_plot, BIC_plot, R2Adj_plot, MSPE_plot,\n                           labels = c('AIC', 'BIC', 'R2Adj', 'MSPE'))\n    \n    print(combined_plot)\n}\n\n\n\nCreate test/train set and Run Custom Model Plotting\n\n\nCodeset.seed(42)\nn = floor(0.8 * nrow(df)) #find the number corresponding to 80% of the data\nindex = sample(seq_len(nrow(df)), size = n) #randomly sample indicies to be included in the training set\n# playoffs\ntrain = df[index, ] #set the training set to be the randomly sampled rows of the dataframe\ntest = df[-index, ] #set the testing set to be the remaining rows\n\n\n\nCodeplot_mlr(full_set=df,\n         train_set=train,\n         test_set=test,\n         response='WP',\n         subset_type='')\n\n\n\n\n\n\n\n  (Intercept)   OBP   SLG    BA   RD Playoffs Champion\n1        TRUE FALSE FALSE FALSE TRUE    FALSE    FALSE\n2        TRUE FALSE FALSE FALSE TRUE     TRUE    FALSE\n3        TRUE FALSE FALSE FALSE TRUE     TRUE     TRUE\n4        TRUE FALSE FALSE  TRUE TRUE     TRUE     TRUE\n5        TRUE  TRUE FALSE  TRUE TRUE     TRUE     TRUE\n6        TRUE  TRUE  TRUE  TRUE TRUE     TRUE     TRUE\n\n\n\n\n\n\n\n\n\nWe can build a final model using the fact that both AIC and MSPE suggest Model 3.\n\n\nCodefinal_lm &lt;- lm(data=df, WP ~ RD + Playoffs + Champion)\nsummary(final_lm)\n\n\nCall:\nlm(formula = WP ~ RD + Playoffs + Champion, data = df)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.088497 -0.016262  0.000326  0.015971  0.075166 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.957e-01  7.905e-04 627.027  &lt; 2e-16 ***\nRD          5.972e-04  8.156e-06  73.220  &lt; 2e-16 ***\nPlayoffs    1.790e-02  2.166e-03   8.263 3.63e-16 ***\nChampion    9.523e-03  3.742e-03   2.545   0.0111 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02366 on 1228 degrees of freedom\nMultiple R-squared:  0.888, Adjusted R-squared:  0.8877 \nF-statistic:  3246 on 3 and 1228 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "model_selection.html#variance-inflation-factors-vif",
    "href": "model_selection.html#variance-inflation-factors-vif",
    "title": "Model Selection",
    "section": "Variance Inflation Factors (VIF)",
    "text": "Variance Inflation Factors (VIF)\n\nFinally, we can test the model for any multicollinearity.\n\n\nCodevif(final_lm)\n\n      RD Playoffs Champion \n1.545945 1.640246 1.246315 \n\n\nNote that all features have a VIF under 5, thus our model does lack evidence suggesting any multicollinearity."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Statistics",
    "section": "",
    "text": "A website dedicated to knowledge obtained for the statistics used in data science."
  },
  {
    "objectID": "gam.html",
    "href": "gam.html",
    "title": "Generalized Addititive Models",
    "section": "",
    "text": "Welcome to the world of GAMs and nonparametric regression."
  },
  {
    "objectID": "gam.html#advantages",
    "href": "gam.html#advantages",
    "title": "Generalized Addititive Models",
    "section": "Advantages",
    "text": "Advantages\n\nflexibility\nfewer distributional assumptions"
  },
  {
    "objectID": "gam.html#disadvantages",
    "href": "gam.html#disadvantages",
    "title": "Generalized Addititive Models",
    "section": "Disadvantages",
    "text": "Disadvantages\n\nless efficient when structure of the relationship is available\ninterpretation difficulties"
  },
  {
    "objectID": "gam.html#marginal-impacts-of-each-feature",
    "href": "gam.html#marginal-impacts-of-each-feature",
    "title": "Generalized Addititive Models",
    "section": "Marginal Impacts of Each Feature",
    "text": "Marginal Impacts of Each Feature\n\ndefined as each feature’s individual relationship with response"
  },
  {
    "objectID": "gam.html#additive-function",
    "href": "gam.html#additive-function",
    "title": "Generalized Addititive Models",
    "section": "Additive Function",
    "text": "Additive Function\n\nmodel: \\(Y_i = f(x_i) + \\epsilon_i\\)\n\n\n\\(k\\): kernel\n\n\\(\\lambda\\): smoothing parameter\n\nAdditive Function\n\\[\\hat{f_{\\lambda}}(x) = \\frac{\\frac{1}{n\\lambda}\\sum\\limits_{i=1}^n K(\\frac{x-x_i}{\\lambda})Y_i}{\\sum\\limits_{i=1}^n K(\\frac{x-x_i}{\\lambda})}\\]"
  },
  {
    "objectID": "gam.html#smoothing-parameter",
    "href": "gam.html#smoothing-parameter",
    "title": "Generalized Addititive Models",
    "section": "Smoothing Parameter",
    "text": "Smoothing Parameter\n\n\n\\(\\lambda_{small}\\): lots of wiggles\n\n\\(\\lambda_{increases}\\): less wiggles, more smoother, can find the “just right”\n\n\\(\\lambda_{too large}\\): too smooth, we risk missing key patterns\nwhen choosing the smoothing parameter, pick the least smooth fit that does not show any implausible fluctuations\n\nGLMs: we know the link function we will use (identity, log, sigmoid)\nGAMs: we don’t know, they are learned from the data"
  },
  {
    "objectID": "gam.html#kernel-estimator",
    "href": "gam.html#kernel-estimator",
    "title": "Generalized Addititive Models",
    "section": "Kernel Estimator",
    "text": "Kernel Estimator\n\nKernel: a nonnegative, real-valued function \\(K\\) such that \\(K(x) = K(-x)\\) for all values of x (i.e. symmetry) and \\(\\int K(x) dx = 1\\) (i.e. normalization).\n\nCommon Kernel Estimators\n\nUniform/Rectangular: \\(K(x) = \\frac{1}{2}\\), \\(-1 \\leq x \\leq 1\\)\n\nGaussian/Normal: \\(K(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\\)\n\nEpanechnikov: \\(K(x) = \\frac{3}{4} (1-x)^2\\), \\(-1 \\leq x \\leq 1\\)"
  },
  {
    "objectID": "gam.html#smoothing-vs.-regression-splines",
    "href": "gam.html#smoothing-vs.-regression-splines",
    "title": "Generalized Addititive Models",
    "section": "Smoothing vs. Regression Splines",
    "text": "Smoothing vs. Regression Splines\nSmoothing Splines\n\nSmoothing splines are used to fit a smooth curve that passes close to the given datapoints.\nThey involve a roughness penalty to ensure the smoothness of the fitted curve. This penalty is an integrated squared second derivative times a smoothing parameter.\nSmoothing splines typically have nkots at each data point, but the roughness penalty prevents overfitting by srhinking the coefficients.\n\nRegression Splines\n\nRegression splines fit a piecewise polynomial function with a reduced set of knots, compared to smoothing splines.\nThey do not use a roughness penalty. Instead, the fit is typically obtained by least squares, which minimizes the sum of squared residuals.\nRegression splines are more about estimating functional relationships rather than transforming variables.\n\nMain Differences\nSmoothing splines are more flexible due to the roughness penalty, while regression splines provide a simpler model with fewer knots and no penalty term. Should be chosen specific for each analysis."
  },
  {
    "objectID": "gam.html#kernel-models",
    "href": "gam.html#kernel-models",
    "title": "Generalized Addititive Models",
    "section": "Kernel Models",
    "text": "Kernel Models\n\nKernel Specific Data\n\n\nCodemarketing = read.csv(\"https://raw.githubusercontent.com/bzaharatos/-Statistical-Modeling-for-Data-Science-Applications/master/Modern%20Regression%20Analysis%20/Datasets/marketing.txt\", sep = \"\")\nhead(marketing)\n\n  youtube facebook newspaper sales\n1  276.12    45.36     83.04 26.52\n2   53.40    47.16     54.12 12.48\n3   20.64    55.08     83.16 11.16\n4  181.80    49.56     70.20 22.20\n5  216.96    12.96     70.08 15.48\n6   10.44    58.68     90.00  8.64\n\n\n\nPlot sales (response) against youtube (predictor), and then fit and overlay a kernel regression\n\n\nKernel: normal\nSmooth Parameter: 5\n\n\nCodewith(marketing, plot(sales~youtube))\nwith(marketing, lines(ksmooth(youtube, sales, 'normal', 5)))\n\n\n\n\n\n\n\n\nKernel: normal\nSmooth Parameter: 50\n\n\nCodewith(marketing, plot(sales~youtube))\nwith(marketing, lines(ksmooth(youtube, sales, 'normal', 50)))\n\n\n\n\n\n\n\nMeasuring Kernel Performance\n\nA function to calculate MSPE from a given parameter (bandwidth). Note that ordering the data is required.\n\n\nCodeoptimize_kernel &lt;- function(train_set, test_set, response, predictor, bandwidth) {\n    test_data &lt;- test_set %&gt;% select(!!sym(predictor), !!sym(response))\n    test_data &lt;- test_data %&gt;% arrange(!!sym(predictor))\n    obs &lt;- test_data[[response]]\n    preds &lt;- ksmooth(x = train_set[[predictor]],\n                     y = train_set[[response]],\n                     'normal',\n                     bandwidth,\n                     x.points = test_set[[predictor]])$y\n    mspe &lt;- mean((obs - preds)^2)\n    return(mspe)\n}"
  },
  {
    "objectID": "gam.html#nonparametric-regressions",
    "href": "gam.html#nonparametric-regressions",
    "title": "Generalized Addititive Models",
    "section": "Nonparametric Regressions",
    "text": "Nonparametric Regressions\n\nCreate a sine wave dataset\n\n\nCodeset.seed(88888)\nn = 150\nx = runif(n, 0, pi/2) \ny = sin(pi*x) + rnorm(n, 0, 0.5) \nplot(y ~ x, main = expression(f(x) == sin(pi*x)), pch = 16, cex=0.8, col = alpha(\"darkgrey\", 0.9))\n\n\n\n\n\n\n\n\nPlot some different kernel smoothing parameters\n\n\nCodeplot(y ~ x, main = expression(f(x) == sin(pi*x)), pch = 16, cex=0.8, col = alpha(\"darkgrey\", 0.9))\nlines(ksmooth(x, y, \"normal\", 0.05))\n\n\n\n\n\n\nCodeplot(y ~ x, main = expression(f(x) == sin(pi*x)), pch = 16, cex=0.8, col = alpha(\"darkgrey\", 0.9))\nlines(ksmooth(x, y, \"normal\", 1))\n\n\n\n\n\n\nCodeplot(y ~ x, main = expression(f(x) == sin(pi*x)), pch = 16, cex=0.8, col = alpha(\"darkgrey\", 0.9))\nlines(ksmooth(x, y, \"normal\", 0.3))\n\n\n\n\n\n\n\n\nUse the 0.3 parameter to make some predictions\n\n\nCodeksmooth(x, y, \"normal\", 0.3, x.points = 0.5)\n\n$x\n[1] 0.5\n\n$y\n[1] 1.062831\n\n\nReplicate ksmooth with a Custom Function\n\nCode# custom function\ncustom_smooth = function(x,y,lambda){\n    f = matrix(NA, ncol = 1, nrow = length(x))\n    for (i in 1:length(x)){\n        f[i] = sum(dnorm((x-x[i])/lambda)*y)/sum(dnorm((x-x[i])/lambda))\n    }\n    s = data.frame(x[order(x)],f[order(x)])\n    return(s)\n}\n\n# plotting with custom function\nplot(y ~ x, main = expression(f(x) == sin(pi*x)), pch = 16, cex=0.8, col = alpha(\"darkgrey\", 0.9))\ns1 = custom_smooth(x, y, 0.1); \ns2 = custom_smooth(x,y, 0.2)\nlines(s1$x,s1$f, type = \"l\", col = \"blue\")\nlines(s2$x,s2$f, type = \"l\", col = \"orange\")\n\n\n\n\n\n\n\nSmoothing Spline Estimator\nUse smooth.spline where spar is the smoothing parameter.\n\nCodeplot(y ~ x, main = expression(f(x) == sin(pi*x)), pch = 16, col = alpha(\"grey\", 0.8))\nlines(smooth.spline(x, y, spar = 0.5))\n\n\n\n\n\n\nCodeplot(y ~ x, main = expression(f(x) == sin(pi*x)), pch = 16, col = alpha(\"grey\", 0.8))\nlines(smooth.spline(x, y, spar = 1))\n\n\n\n\n\n\n\nImplement Loess Fit\nUse geom_smooth\n\nCoden = 50; x = runif(n, 0 , pi/2); y = sin(pi*x) + rnorm(n, 0, 2)\ndf = data.frame(x = x, y = y)\nggplot(df)+ \ngeom_point(aes(x = x, y = y)) + \ngeom_smooth(aes(x = x, y = y)) + \ntheme_bw()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "gam.html#other-non-parametric-regressions-and-3-d-plotting",
    "href": "gam.html#other-non-parametric-regressions-and-3-d-plotting",
    "title": "Generalized Addititive Models",
    "section": "Other Non-Parametric Regressions and 3-d Plotting",
    "text": "Other Non-Parametric Regressions and 3-d Plotting\n\nCodedata(savings, package=\"faraway\")\nhead(savings)\n\n             sr pop15 pop75     dpi ddpi\nAustralia 11.43 29.35  2.87 2329.68 2.87\nAustria   12.07 23.32  4.41 1507.99 3.93\nBelgium   13.17 23.80  4.43 2108.47 3.82\nBolivia    5.75 41.89  1.67  189.13 0.22\nBrazil    12.88 42.19  0.83  728.47 4.56\nCanada     8.79 31.72  2.85 2982.88 2.43\n\n\n\nsm package is for “Smoothing Methods for Nonparametric Regression and Density Estimation”\n\n\nCode# The savings rate will be our response variable\ny = savings$sr\n# pop15 and ddpi will be our two predictor variables\nx = cbind(savings$pop15, savings$ddpi)\n\n#sm.regression - usage: sm.regression(x, y, h, design.mat = NA, model = \"none\", weights = NA,\n#group = NA, ...)\nsm.regression(x,y,h=c(1,1),xlab=\"pop15\",ylab=\"growth\",zlab=\"savings rate\")\n\n\n\n\n\n\nCodesm.regression(x,y,h=c(5,5),xlab=\"pop15\",ylab=\"growth\",zlab=\"savings rate\")\n\n\n\n\n\n\n\n\nProduce a spline surface with the gam() Function\n\n\nCodeamod = gam(sr ~ s(pop15,ddpi), data=savings)\nvis.gam(amod, col=\"gray\",ticktype=\"detailed\",theta=-35)\n\n\n\n\n\n\n\n\nloess function\n\n\nCodelomod = loess(sr ~ pop15 + ddpi, data=savings)\nxg = seq(21,48,len=20)\nyg = seq(0,17,len=20)\nzg = expand.grid(pop15=xg,ddpi=yg)\npersp(xg,yg,predict(lomod,zg),theta=-35,ticktype=\"detailed\",xlab=\"pop15\",ylab=\"growth\", zlab = \"savings rate\", col=\"gray\")"
  },
  {
    "objectID": "gam.html#gams",
    "href": "gam.html#gams",
    "title": "Generalized Addititive Models",
    "section": "GAMs",
    "text": "GAMs\n\nCodedata(exp)\n\nWarning in data(exp): data set 'exp' not found\n\nCodehead(exa)\n\n       x       y m\n1 0.0048 -0.0339 0\n2 0.0086  0.1654 0\n3 0.0117  0.0245 0\n4 0.0170  0.1784 0\n5 0.0261 -0.3466 0\n6 0.0299 -0.7550 0\n\nCodeplot(y ~ x, data = exa, main = \"f(x) = sin^3(2pi x^2)\")\n\n\n\n\n\n\n\n\nFirst, attempt a fit with kernel estimators of the unknown function \\(Y = f(x)\\).\n\n\nCodeplot(y~x, data=exa, main=\"f(x) = sin^3(2*pi*x^2)\")\nlines(ksmooth(exa$x, exa$y, 'normal', 0.25))\n\n\n\n\n\n\n\nSmoothing Spline vs. Regression Spline\n\nUse a smoothing spline and a regression spline\n\n\nCode# default is spar=NULL\nplot(y ~ x, data = exb, main = \"f(x) = 0\")\nlines(smooth.spline(exb$x, exb$y))\n\n\n\n\n\n\nCodeplot(y ~ x, data = exb, main = \"f(x) = 0\")\nlines(smooth.spline(exb$x, exb$y, spar = 1))\n\n\n\n\n\n\n\nSimulated Data\n\nCodeset.seed(12)\n\n# construct predictors \nn &lt;- 100\nd &lt;- data.frame(\n    x1=rnorm(n, mean = 45, sd = 15),\n    x2=sample(c('s','m','t'), size=n, replace=TRUE),\n    x3=sample(c(F,T), size=n, replace=TRUE),\n    stringsAsFactors=F)\n\nhead(d)\n\n        x1 x2    x3\n1 22.79149  t FALSE\n2 68.65754  s  TRUE\n3 30.64883  s FALSE\n4 31.19992  s  TRUE\n5 15.03537  t FALSE\n6 40.91556  s FALSE\n\n\n\nFor this example, we make the response some nonlinear/nonparametric function of x1. In a realworld situation, we wouldn’t know this relationship and would estimate it. Other terms are modeled parametrically. The resposne has normal noise. The model we want is a Poisson GAM, with true relationship \\(log(\\mu_i) = \\beta_1 + log(0.5x_i^2) - x_2 + x_3\\)\n\n\nCode# make predictor\nd$mu &lt;- with(d, exp(log(0.5*x1^2)) - as.integer(as.factor(x2)) + as.integer(as.factor(x3)))\nd$y &lt;- rpois(n, d$mu) # manufacturing a poisson response\n\n\n\nCodemod_gam &lt;- gam(y ~ s(x1) + as.integer(as.factor(x2)) + as.integer(as.factor(x3)), data=d, family=poisson)\nmod_gam\n\n\nFamily: poisson \nLink function: log \n\nFormula:\ny ~ s(x1) + as.integer(as.factor(x2)) + as.integer(as.factor(x3))\n\nEstimated degrees of freedom:\n8.64  total = 11.64 \n\nUBRE score: -0.1023909     \n\nCodesummary(mod_gam)\n\n\nFamily: poisson \nLink function: log \n\nFormula:\ny ~ s(x1) + as.integer(as.factor(x2)) + as.integer(as.factor(x3))\n\nParametric coefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               6.798166   0.012954 524.799   &lt;2e-16 ***\nas.integer(as.factor(x2)) 0.001226   0.004055   0.302    0.762    \nas.integer(as.factor(x3)) 0.003369   0.006704   0.503    0.615    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n        edf Ref.df Chi.sq p-value    \ns(x1) 8.643  8.958  28056  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.998   Deviance explained = 99.8%\nUBRE = -0.10239  Scale est. = 1         n = 100\n\nCodeplot(mod_gam)\n\n\n\n\n\n\nCoderes = residuals(mod_gam, response = 'deviance')\nplot(log(predict(mod_gam, type = 'link')), res)\nabline(h=0)\n\n\n\n\n\n\n\n\nCodegam.check(mod_gam)\n\n\n\n\n\n\n\n\nMethod: UBRE   Optimizer: outer newton\nfull convergence after 8 iterations.\nGradient range [4.56651e-09,4.56651e-09]\n(score -0.1023909 & scale 1).\nHessian positive definite, eigenvalue range [0.004773368,0.004773368].\nModel rank =  12 / 12 \n\nBasis dimension (k) checking results. Low p-value (k-index&lt;1) may\nindicate that k is too low, especially if edf is close to k'.\n\n        k'  edf k-index p-value\ns(x1) 9.00 8.64    1.02    0.51\n\n\nUse GAMs on Another Dataset\n\nCodedata(fat)\nhead(fat)\n\n  brozek siri density age weight height adipos  free neck chest abdom   hip\n1   12.6 12.3  1.0708  23 154.25  67.75   23.7 134.9 36.2  93.1  85.2  94.5\n2    6.9  6.1  1.0853  22 173.25  72.25   23.4 161.3 38.5  93.6  83.0  98.7\n3   24.6 25.3  1.0414  22 154.00  66.25   24.7 116.0 34.0  95.8  87.9  99.2\n4   10.9 10.4  1.0751  26 184.75  72.25   24.9 164.7 37.4 101.8  86.4 101.2\n5   27.8 28.7  1.0340  24 184.25  71.25   25.6 133.1 34.4  97.3 100.0 101.9\n6   20.6 20.9  1.0502  24 210.25  74.75   26.5 167.0 39.0 104.5  94.4 107.8\n  thigh knee ankle biceps forearm wrist\n1  59.0 37.3  21.9   32.0    27.4  17.1\n2  58.7 37.3  23.4   30.5    28.9  18.2\n3  59.6 38.9  24.0   28.8    25.2  16.6\n4  60.1 37.3  22.8   32.4    29.4  18.2\n5  63.2 42.2  24.0   32.2    27.7  17.7\n6  66.0 42.0  25.6   35.7    30.6  18.8\n\n\n\nCode# want to determining if we should use the smoothing function on each of the features\ngam_mod &lt;- gam(siri ~ s(weight) + s(height) + s(chest) + s(neck) + s(abdom) + s(hip) + s(thigh) + s(knee) + s(ankle) + s(biceps) + s(forearm) + s(wrist), data=fat)\n\n# res vs predicted\nres &lt;- residuals(gam_mod, type='deviance')\nplot(log(predict(gam_mod, type='response')), res)\nabline(h=0)\n\n\n\n\n\n\nCode# qqplot\nqqnorm(res)\n\n\n\n\n\n\nCode# missed a plot - SEE LECTURE VERSION\n# maybe\nplot.gam(gam_mod)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodedata(ozone)\nhead(ozone)\n\n  O3   vh wind humidity temp  ibh dpg ibt vis doy\n1  3 5710    4       28   40 2693 -25  87 250  33\n2  5 5700    3       37   45  590 -24 128 100  34\n3  5 5760    3       51   54 1450  25 139  60  35\n4  6 5720    4       69   35 1568  15 121  60  36\n5  4 5790    6       19   45 2631 -33 123 100  37\n6  4 5790    3       25   55  554 -28 182 250  38\n\n\n\nCodegam_ozone &lt;- gam(O3 ~ s(temp) + s(ibh) + s(ibt), data=ozone)\nsummary(gam_ozone)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nO3 ~ s(temp) + s(ibh) + s(ibt)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.7758     0.2382   49.44   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n          edf Ref.df      F  p-value    \ns(temp) 3.386  4.259 20.681  &lt; 2e-16 ***\ns(ibh)  4.174  5.076  7.338 1.74e-06 ***\ns(ibt)  2.112  2.731  1.400    0.214    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.708   Deviance explained = 71.7%\nGCV = 19.346  Scale est. = 18.72     n = 330\n\n\n\nCodeplot.gam(gam_ozone)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: IBT has evidence that it could be linear, while others do not. What we want to see here is a linear line through the confidence bounds."
  },
  {
    "objectID": "confidence_intervals.html",
    "href": "confidence_intervals.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "This page features confidence intervals’ definition, interpretation, and programming."
  },
  {
    "objectID": "confidence_intervals.html#ci-for-means",
    "href": "confidence_intervals.html#ci-for-means",
    "title": "Confidence Intervals",
    "section": "CI for Means",
    "text": "CI for Means\nThe following example is for a \\(100 * (1 - \\alpha) \\%\\) confidence interval for mean \\(\\mu\\) when the value of \\(\\sigma\\) is known:\nLarge Sample CI for a Mean (Z-test)\n\\([\\bar{x} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\bar{x} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}]\\)\n\\(= \\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\)\nSmall Sample CI for a Mean (t-test)\n\\(= \\bar{x} \\pm t_{n-1, \\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\)"
  },
  {
    "objectID": "confidence_intervals.html#ci-for-difference-in-means",
    "href": "confidence_intervals.html#ci-for-difference-in-means",
    "title": "Confidence Intervals",
    "section": "CI for Difference in Means",
    "text": "CI for Difference in Means\nLarge Sample CI for a Mean (Z-test)\n\\(\\bar{x} - \\bar{y} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\sigma_{x}^{2}}{n_x} + \\frac{\\sigma_{y}^{2}}{n_y}}\\)\nSmall Sample CI for a Mean (t-test)\n\\(\\bar{x} - \\bar{y} \\pm t \\sqrt{\\frac{\\sigma_{x}^{2}}{n_x} + \\frac{\\sigma_{y}^{2}}{n_y}}\\)\nwhere\n\\(t = t_{n_x + n_y - 2, \\alpha/2}\\)"
  },
  {
    "objectID": "confidence_intervals.html#ci-for-proportions",
    "href": "confidence_intervals.html#ci-for-proportions",
    "title": "Confidence Intervals",
    "section": "CI for Proportions",
    "text": "CI for Proportions\n\\(p \\pm \\pm z_{\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}}\\)"
  },
  {
    "objectID": "confidence_intervals.html#ci-for-difference-in-proportions",
    "href": "confidence_intervals.html#ci-for-difference-in-proportions",
    "title": "Confidence Intervals",
    "section": "CI for Difference in Proportions",
    "text": "CI for Difference in Proportions\n\\((\\hat{p_1} - \\hat{p_2}) \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p_1}(1-\\hat{p_1})}{n_1} + \\frac{\\hat{p_2}(1-\\hat{p_2})}{n_2}}\\)"
  },
  {
    "objectID": "confidence_intervals.html#ci-for-slope-distribution-betas-in-regression-models",
    "href": "confidence_intervals.html#ci-for-slope-distribution-betas-in-regression-models",
    "title": "Confidence Intervals",
    "section": "CI for Slope Distribution (\\(\\beta\\)s in Regression Models)",
    "text": "CI for Slope Distribution (\\(\\beta\\)s in Regression Models)\n\\(\\hat{\\beta} \\pm t_{\\alpha / 2, df = n-2} SE(\\hat{\\beta})\\)\nwhere\n\\(SE(\\hat{\\beta}) = \\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}}\\)"
  },
  {
    "objectID": "anova_categorical.html",
    "href": "anova_categorical.html",
    "title": "ANOVA for Categorical",
    "section": "",
    "text": "We can use the f-distribution for ANOVAs (Analysis of Variance), which we’ve seen can be used to compare models, but they are also great for categorical variables."
  },
  {
    "objectID": "anova_categorical.html#degrees-of-freedom",
    "href": "anova_categorical.html#degrees-of-freedom",
    "title": "ANOVA for Categorical",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nGiven \\(I\\) number of groups/categories and \\(N\\) total number of data points (across all groups/categories; recall that groups can have different number of data points):\n\n\\(SSB_{df} = I-1\\)\n\\(SSW_{df} = N-I\\)"
  },
  {
    "objectID": "anova_categorical.html#f-statistic",
    "href": "anova_categorical.html#f-statistic",
    "title": "ANOVA for Categorical",
    "section": "F-Statistic",
    "text": "F-Statistic\nGiven:\n\n\n\\(H_0\\): \\(\\mu_0 = \\mu_1 = \\mu_2\\)\n\n\n\\(H_a\\): \\(\\mu_i \\neq \\mu_j\\) for some pair \\(i, j\\)\n\n\n\\(F_{stat} = \\frac{SSB/SSB_{df}}{SSW/SSW_{df}} = \\frac{\\frac{SSB}{I-1}}{\\frac{SSW}{N-I}}\\)\nRejection Region: \\(F_{\\alpha,I-1,N-I}\\)"
  },
  {
    "objectID": "anova_categorical.html#anova-table",
    "href": "anova_categorical.html#anova-table",
    "title": "ANOVA for Categorical",
    "section": "ANOVA Table",
    "text": "ANOVA Table\nIt’s common to summarize everything associated with the analysis of variance in a table.\nGiven the example:\n\n\nControl\nDiet A\nDiet B\n\n\n\n3\n5\n5\n\n\n2\n3\n6\n\n\n1\n4\n7\n\n\n\n\n\nANOVA\nSS\nDF\nSS/DF\n\\(F_{stat}\\)\n\n\n\nbetween\n24\n2\n12\n12\n\n\nwithin\n6\n6\n1\n\n\n\nbetween\n30"
  },
  {
    "objectID": "anova_categorical.html#the-test",
    "href": "anova_categorical.html#the-test",
    "title": "ANOVA for Categorical",
    "section": "The Test",
    "text": "The Test\nSuppose we determine that some of the means are different. How can we tell which ones?\nTukey’s Honest Significance Test (HST/HSD) post-hoc test for iff we reject the null hypothesis\n\nHypothesis test for pairwise comparison of means: many tests using what’s called the studentized range distribution\nAdjusts so that the probability of making a Type I error over all possible pairwise comparisons is kept at the original \\(\\alpha\\)\n\n\nWe can draw a conclusion from both the confidence interval and p-values output from this method (given a prescribed \\(\\alpha\\).\n\nconfidence interval [lower bound, upper bound]: if the lower bound and upper bound do not contain \\(0\\), we know that the difference between these groups is significant. In particular, we can know if the difference is positive, since the lower bound of the confidence interval is greater than \\(0\\).\np-value: given a confidence interval which does not contain \\(0\\), we would likely see a small enough p-value to reject the null hypothesis, indicating that the difference between the groups’ means is statistically significant.\n\nIn other words, we’d be doing the same test as before, just pairwise:\n\n\n\\(H_0\\): \\(\\mu_i = \\mu_j = \\mu_C = \\dots\\) for two groups/categories \\(i, j\\)\n\n\n\\(H_a\\): \\(\\mu_i \\neq \\mu_j\\) for two groups/categories \\(i, j\\)"
  },
  {
    "objectID": "anova_categorical.html#one-way-anova-test",
    "href": "anova_categorical.html#one-way-anova-test",
    "title": "ANOVA for Categorical",
    "section": "One-Way ANOVA Test",
    "text": "One-Way ANOVA Test\n\nPerform Oneway ANOVA Test (var.equal = TRUE for this test, Var.equal = FALSE is another test)\n\n\nCoderesult &lt;- oneway.test(value ~ group, data = longdata, var.equal = TRUE)\n\n\n\nThe Results\n\n\nCodeFstat = result$statistic\npvalue = result$p.value\n\nFstat\n\n F \n12 \n\nCodepvalue\n\n[1] 0.008"
  },
  {
    "objectID": "anova_categorical.html#anova-as-mlr-1",
    "href": "anova_categorical.html#anova-as-mlr-1",
    "title": "ANOVA for Categorical",
    "section": "ANOVA as MLR",
    "text": "ANOVA as MLR\nWe get a low enough p-value that we have enough evidence to reject the null hypothesis and conclude that at least one mean is different.\n\nWhat about the idea that ANOVA F-test is equivalent to linear regression where the features are binary categorical variables associated with group membership. We can recreate the 0, 1 dataframe.\n\n\nCode# x1: indicates membership in diet a\n# x2: indicates membership in diet b\n\nx1 &lt;- as.integer(longdata$group == 'DietA')\nx2 &lt;- as.integer(longdata$group == 'DietB')\ny &lt;- longdata$value\n\ny\n\n[1] 3 2 1 5 3 4 5 6 7\n\nCodex1\n\n[1] 0 0 0 1 1 1 0 0 0\n\nCodex2\n\n[1] 0 0 0 0 0 0 1 1 1\n\nCodedfRegression &lt;- data.frame(y, x1, x2)\ndfRegression\n\n  y x1 x2\n1 3  0  0\n2 2  0  0\n3 1  0  0\n4 5  1  0\n5 3  1  0\n6 4  1  0\n7 5  0  1\n8 6  0  1\n9 7  0  1\n\n\n\nPerform MLR and compare the computed F-test\n\n\nCodeanova_regression &lt;- lm(y ~ ., data = dfRegression)\nsummary(anova_regression)\n\n\nCall:\nlm(formula = y ~ ., data = dfRegression)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n    -1     -1      0      1      1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   2.0000     0.5774   3.464  0.01340 * \nx1            2.0000     0.8165   2.449  0.04983 * \nx2            4.0000     0.8165   4.899  0.00271 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 6 degrees of freedom\nMultiple R-squared:    0.8, Adjusted R-squared:  0.7333 \nF-statistic:    12 on 2 and 6 DF,  p-value: 0.008"
  },
  {
    "objectID": "anova_categorical.html#analysis-of-variance-summary-table",
    "href": "anova_categorical.html#analysis-of-variance-summary-table",
    "title": "ANOVA for Categorical",
    "section": "Analysis of Variance Summary Table",
    "text": "Analysis of Variance Summary Table\n\nUse aov() to create a summary table from original long data\n\n\nCodeweightloss_study_aov &lt;- aov(value ~ group, data = longdata)\nsummary(weightloss_study_aov)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)   \ngroup        2     24      12      12  0.008 **\nResiduals    6      6       1                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "anova_categorical.html#tukey-test",
    "href": "anova_categorical.html#tukey-test",
    "title": "ANOVA for Categorical",
    "section": "Tukey Test",
    "text": "Tukey Test\n\nUse TukeyHSD() on the aov summary to test which weight loss group(s) are statistically different\n\n\nCode# NOTE: can set significance level, i.e. TukeyHSD(aov_model, conf.level=0.95)\ntukey_test &lt;- TukeyHSD(weightloss_study_aov)\ntukey_test\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = value ~ group, data = longdata)\n\n$group\n              diff        lwr      upr     p adj\nDietA-Control    2 -0.5052356 4.505236 0.1088670\nDietB-Control    4  1.4947644 6.505236 0.0064937\nDietB-DietA      2 -0.5052356 4.505236 0.1088670\n\n\n\nPlotting the Tukey Test Results\n\n\nCodeplot(tukey_test)"
  },
  {
    "objectID": "anova_categorical.html#checking-assumptions",
    "href": "anova_categorical.html#checking-assumptions",
    "title": "ANOVA for Categorical",
    "section": "Checking Assumptions",
    "text": "Checking Assumptions\nSpecifically, we want to check:\n\nhomogeniety of variance assumption\nnormality assumption\n\n\nPlotting\n\n\nCodeplot(weightloss_study_aov)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevenes’ Test (constant variance)\n\n\nCodeleveneTest(value ~ group, data = longdata)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2       0      1\n       6               \n\n\n\n\n\\(H_0\\): homogeneity of variance across all groups\n\n\\(H_a\\): homogeneity of variance condition violated\n\n\nShapiro-Wilk’s test (normality)\n\n\nCodeaov_residuals &lt;- residuals(object = weightloss_study_aov)\nshapiro.test(x = aov_residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  aov_residuals\nW = 0.82304, p-value = 0.03729"
  },
  {
    "objectID": "anova_categorical.html#what-if-assumptions-are-not-met",
    "href": "anova_categorical.html#what-if-assumptions-are-not-met",
    "title": "ANOVA for Categorical",
    "section": "What if Assumptions are not Met?",
    "text": "What if Assumptions are not Met?\nWhen assumptions are not met, we can use a non-parametric test known as the Kruskal-Wallis rank sum test, which determines whether or not there is a statistically significant difference between the medians of three or more independent groups. Note this returns a chi-squared value.\n\n\n\\(H_0\\): the median is equal across all groups\n\n\\(H_a\\): the median is not equal across all groups\n\n\nCodekruskal.test(value ~ group, data = longdata)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  value by group\nKruskal-Wallis chi-squared = 6.5311, df = 2, p-value = 0.03818\n\n\nWe can further determine which groups have significant differences between them by using pairwise.wilcox.test()."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\nCode1 + 1\n\n[1] 2"
  },
  {
    "objectID": "bayes_nets.html",
    "href": "bayes_nets.html",
    "title": "Bayesian Networks",
    "section": "",
    "text": "The fundamental issue of causal inference is that causal effects cannot be measured directly.\nCausal Diagram:\n\ngraphical representation of data generating process (DGP):\n\nnode: variables in the DGP\narrows: show direction of causation\n\nlarge number of test subjects can “skew” proportions"
  },
  {
    "objectID": "bayes_nets.html#example",
    "href": "bayes_nets.html#example",
    "title": "Bayesian Networks",
    "section": "Example",
    "text": "Example\nCausal Diagram: \\(Rain \\rightarrow Traffic\\)\nBayesian Network: \\(Rain \\rightarrow Traffic\\)\n\nwith a Bayes Net, the arrow does not necessarily indicate causality\ninstead, it is indicating the child node has a conditionally dependent relationship with the parent node and is conditionally independent of non-parent nodes (naive assumption)."
  },
  {
    "objectID": "bayes_nets.html#the-bayesian-network",
    "href": "bayes_nets.html#the-bayesian-network",
    "title": "Bayesian Networks",
    "section": "The Bayesian Network",
    "text": "The Bayesian Network\nThe point of a Bayes Net is to represent full joint probability distributions, and to encode an interrelated set of conditional independence/probability statements.\n\nnodes (events)\nconditional probability tables (CPTs), relating those events\ndescribe how variables interact locally\nchain together local interactions to estimate global, indirect interactions\n\n\nTo put another way, Bayes Nets implicitly encode joint distributions as a product of the local conditional probabilities.\n\n\\(P(x_1, x_2, \\dots, x_n) = \\prod\\limits_{i=1}^n P(x_i | x_{i-1}, x_{i-2}, \\dots, x_1\\)\nKeeping in mind that each node is conditionally independent of its other predecessors, given its parents, we can order in such a way that:\n\\(\\prod\\limits_{i=1}^n P(x_i | x_{i-1}, x_{i-2}, \\dots, x_1) = \\prod\\limits_{i=1}^n P(x_i | parents(X_i))\\)"
  },
  {
    "objectID": "diagnostics.html",
    "href": "diagnostics.html",
    "title": "Diagnostics",
    "section": "",
    "text": "Aside from metrics like t-test p-values, f-test p-values, \\(R^2\\), and \\(R^2_a\\), how can we determine if the model is “good”?\nFor most models, especially linear, we want to confirm the following main assumptions:\n\nLinearity:\n\nusing the observed vs. predicted plot, we want to observe a linear trend\n\n\nIndependence:\n\nusing the successive residuals plot, we want to observe constant variance around the origin\n\n\nConstant Variance (homoscedasticity)\n\nin the residual vs. fitted plot, we want to observe randomness about the x-axis. Signs of non-constant variance (heteroscedasticity) or issue with the structure of the data include discernible patterns such as curves or a cone-like pattern. To clarify, a pattern may indicate the underlying model/data is wrong while the widening of residuals could indicate non-constant variance.\nin the scale-location plot, if the average residual value is increasing (or decreasing) with the fitted values, this also indicates heteroscedasticity. There’s usually a line representing the average residual for each fitted value. If this has a slope, this is a sign of non-constant variance. The most ideal value would be a completely flat line.\n\n\nNormality:\n\nin the qq-plot, we want to observe a generally straight diagonal line (i.e. \\(y=x\\)).\nnormality of residuals improves accuracy and confidence in model fit\nthe inference in \\(\\beta_j\\)s relies upon the assumption of normality\nevery term has some degree of error, recall that we assume \\(\\epsilon_j \\approx N(0, \\sigma^2)\\) (which actually yields that we assume errors are independent, have constant variance, and are normally distributed)\n\n\n\nAdditionally, we want to address:\n\nOutliers (influential points):\n\nin the residuals vs. leverage plot, we don’t want to see any points crossing the Cook’s distance thresholds. There are different levels of thresholds, and each successive threshold indicates outliers in that area have even more influence than the previous threshold."
  },
  {
    "objectID": "diagnostics.html#plotting",
    "href": "diagnostics.html#plotting",
    "title": "Diagnostics",
    "section": "Plotting",
    "text": "Plotting\nThe built-in function to display a few of these plots:\nplot(model)\nThis will generally output:\n\n\nResiduals vs. Fitted (constant variance check)\n\nQQ-Plot (normality check)\n\nScale-Location (constant variance check)\n\nResiduals vs. Leverage (outliers)\n\nWe need to manually create a few plots to check the other assumptions:\nFor linear models:\n\nCode# predicted vs. observed\nplot(predict(model), df[response_variable], main='Predicted vs. Observed', xlab='Predicted', ylab='Observed')\n\n# successive residuals\ndf_diagnostics = data.frame(yhat = fitted(model),\n                            r = resid(model),\n                            y = df[[response_var]])\nn = dim(df)[1]; \nx = head(df_diagnostics$r, n-1)\ny = tail(df_diagnostics$r, n-1)\nsrp = data.frame(x,y)\nggplot(srp, aes(x = x, y = y)) + \n    geom_point() + \n    geom_vline(xintercept = 0) + \n    geom_hline(yintercept = 0) + \n    xlab(expression(hat(epsilon)[i])) +\n    ylab(expression(hat(epsilon)[i+1])) + \n    ggtitle(\"Successive Residual Plot\") + \n    theme_bw() + \n    theme(plot.title = element_text(hjust = 0.5))\n\n\nThis will output:\n\n\nPredicted vs. Observed (linearity check)\n\nSuccessive Residuals Plot (independence check)"
  },
  {
    "objectID": "diagnostics.html#hypothesis-tests",
    "href": "diagnostics.html#hypothesis-tests",
    "title": "Diagnostics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\n\nShapiro-Wilks Test:\n\ntests for normality\n\n\n\\(H_0\\): residuals are normal\n\n\\(H_a\\): residuals are not normal\ni.e. with an acceptably low p-value, we would reject the null hypothesis indicating there is enough evidence to suggest the residuals are not normal. a “good” model would have a higher p-value, suggesting normal residuals.\n\n\nshapiro.test(residuals)\n\n\nDurbin-Watson Test:\n\ntest for correlation of errors between successive residuals\n\n\n\\(H_0\\): the errors are uncorrelated\n\n\\(H_a\\): the errors are correlated (specifically, the residuals exhibit autocorrelation)\ni.e. with an acceptably low p-value, we would reject the null hypothesis indicated that there is enough evidence to suggest the residuals are correlated. a “good” model would have a higher p-value, suggesting uncorrelated residuals.\n\n\nfrom package car, durbinWatsonTest(model)\n\n\n\nLevenes’ Test:\n\ntests for homogeneity of variance\n\n\n\\(H_0\\): the variance among the groups is equal\n\n\\(H_a\\): the variance among the groups is not equal"
  },
  {
    "objectID": "diagnostics.html#full-diagnostic-plots-function",
    "href": "diagnostics.html#full-diagnostic-plots-function",
    "title": "Diagnostics",
    "section": "Full Diagnostic Plots Function",
    "text": "Full Diagnostic Plots Function\n\nNote that this includes an option for linear and generalized linear models\n\n\nCode# diagnostics\ndiagnostics_plots &lt;- function(df, model, response_var, type='linear') {\n    if (type=='linear') {\n        # default plots\n        plot(model)\n        # predicted vs observed\n        plot(predict(model), df[[response_var]],\n             main='Predicted vs Observed',\n             xlab='Predicted', ylab='Observed')\n        # successive residuals\n        df_diagnostics = data.frame(yhat = fitted(model),\n                                    r = resid(model),\n                                    y = df[[response_var]])\n        n = dim(df)[1]; \n        x = head(df_diagnostics$r, n-1)\n        y = tail(df_diagnostics$r, n-1)\n        srp = data.frame(x,y)\n        ggplot(srp, aes(x = x, y = y)) + \n            geom_point() + \n            geom_vline(xintercept = 0) + \n            geom_hline(yintercept = 0) + \n            xlab(expression(hat(epsilon)[i])) +\n            ylab(expression(hat(epsilon)[i+1])) + \n            ggtitle(\"Successive Residual Plot\") + \n            theme_bw() + \n            theme(plot.title = element_text(hjust = 0.5))\n    } else  if (type=='response') {\n        # default plots\n        plot(model)\n        # predicted vs observed\n        plot(predict(model, type='response'), df[[response_var]],\n             main='Predicted vs Observed',\n             xlab='Predicted', ylab='Observed')\n        # contingency table\n        df_diagnostics &lt;- df\n        predicted_value &lt;- predict(model, df, type='response')\n        df_diagnostics$Predicted &lt;- ifelse(predicted_value &gt; 0.5, 1, 0)\n        contingency_table &lt;- table(df_diagnostics[[response_var]],\n                                   df_diagnostics$Predicted,\n                                   dnn=c(response_var, 'Predicted'))\n        accuracy &lt;- sum(diag(contingency_table)) / sum(contingency_table)\n        print(accuracy)\n        print(contingency_table)\n        # successive residuals\n        df_diagnostics = data.frame(yhat = fitted(model),\n                                    r = resid(model),\n                                    y = df[[response_var]])\n        n = dim(df)[1]; \n        x = head(df_diagnostics$r, n-1)\n        y = tail(df_diagnostics$r, n-1)\n        srp = data.frame(x,y)\n        ggplot(srp, aes(x = x, y = y)) + \n            geom_point() + \n            geom_vline(xintercept = 0) + \n            geom_hline(yintercept = 0) + \n            xlab(expression(hat(epsilon)[i])) +\n            ylab(expression(hat(epsilon)[i+1])) + \n            ggtitle(\"Successive Residual Plot\") + \n            theme_bw() + \n            theme(plot.title = element_text(hjust = 0.5))\n    } else {\n        print('Please use either type: linear or type: response')\n    }\n}"
  },
  {
    "objectID": "glm.html",
    "href": "glm.html",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "We can address a larger amount of problems and models by abstracting a general for of linear models to account for categorical responses types such as binomial, multinomial, and poisson.\nA GLM is defined by specifying two components:\n\nresponse: member of the exponential family distribution\nlink function: describes how the mean of the response and a linear combination of the predictors are related"
  },
  {
    "objectID": "glm.html#common-glms",
    "href": "glm.html#common-glms",
    "title": "Generalized Linear Models",
    "section": "Common GLMs",
    "text": "Common GLMs\n\nBinomial Distribution\nPoisson Distribution"
  },
  {
    "objectID": "glm.html#glm-setup",
    "href": "glm.html#glm-setup",
    "title": "Generalized Linear Models",
    "section": "GLM Setup",
    "text": "GLM Setup\nLet\n\n\n\\(x_j = (x_{1, j}, \\dots x_{n, j})^T\\), \\(j=1, \\dots, p\\) be a set of predictors\n\\(Y = (Y_{1}, \\dots Y_{n})^T\\)\n\\(\\beta = (\\beta_0, \\beta_1, \\dots, \\beta_p)^T\\)\n\nA GLM has three components:\n\nRandom Component:\n\nrefers to the probability distribution of the response variable (i.e. binomial distribution)\na random variable from the exponential family if the distribution can be written as the exponential function above\n\n\nSystematic Component:\n\nrefers to the explanatory variables \\((X_1, \\dots, X_k)\\) as a combination of linear predictors\n\\(\\eta = \\beta_0 + \\beta_1 x_1 + \\dots \\beta_p x_p = x^T\\beta\\)\n\n\nLink Function:\n\nspecifies the link between the random and systematic components\ndescribes how the expected value of the response relates to the linear predictor of explanatory variables\nlink function, \\(g\\), describes how the mean response, \\(E[Y] = \\mu\\) is linked to the covariates through the linear predictor: \\(\\eta = g(\\mu)\\)\n\ni.e. \\(\\eta = logit(\\pi)\\) as in logistic regression\nwe require a monotone continuous and differentiable function"
  },
  {
    "objectID": "glm.html#log-likelihood",
    "href": "glm.html#log-likelihood",
    "title": "Generalized Linear Models",
    "section": "Log-Likelihood",
    "text": "Log-Likelihood\n\nlog-likelihood function: \\(l(\\theta) = \\frac{y\\theta - b(\\theta)}{a(\\phi) + c(y, \\phi)}\\)\n\nderivative wrt \\(\\theta\\): \\(l'(\\theta) = \\frac{y - b'(\\theta)}{a(\\phi)}\\)\n\nexpectation over \\(y\\): \\(E[l'(\\theta)] = \\frac{E[y] - b'(\\theta)}{a(\\phi)}\\)\n\n\nFrom general likelihood theory, we know that \\(E[l'(\\theta)] = 0 \\rightarrow E[Y] = \\mu = b'(\\theta)\\)\n\nsecond derivative: \\(l''(\\theta) = -E[(l'(\\theta))^2]\\)\n\n\\(\\frac{b''(\\theta)}{a(\\phi)} = \\frac{E[(Y - b'(\\theta))^2]}{a^2(\\phi)}\\)\nresults in: \\(var(Y) = b''(\\theta)a(\\phi)\\)"
  },
  {
    "objectID": "glm.html#comparison-to-linear-regresion",
    "href": "glm.html#comparison-to-linear-regresion",
    "title": "Generalized Linear Models",
    "section": "Comparison to Linear Regresion",
    "text": "Comparison to Linear Regresion\n\n\n\n\n\n\n\n\nType\nRandom Component\nSystematic Component\nLink Function\n\n\n\nLinear Regression\n\\(Y \\sim Normal\\)\n\\(\\eta=\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\\)\nIdentity\n\n\nLogistic Regression\n\\(Y \\sim Binomial\\)\n\\(\\eta=\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\\)\nlogit: \\(\\eta = log(\\frac{p}{1-p})\\)"
  },
  {
    "objectID": "glm.html#deviance-of-binomial-regression",
    "href": "glm.html#deviance-of-binomial-regression",
    "title": "Generalized Linear Models",
    "section": "Deviance of Binomial Regression",
    "text": "Deviance of Binomial Regression\n\\(D = 2 \\sum\\limits_{i=1}^n (y_i log(\\frac{y_i}{\\hat{y_i}}) + (n_i - y_i)log(\\frac{n_i - y_i}{n_i - \\hat{y_i}}))\\)"
  },
  {
    "objectID": "glm.html#deviance-of-poisson-regression",
    "href": "glm.html#deviance-of-poisson-regression",
    "title": "Generalized Linear Models",
    "section": "Deviance of Poisson Regression",
    "text": "Deviance of Poisson Regression\n\\(D = -2l(\\hat{\\beta})\\)\n\\(= -2 \\sum\\limits_{i=1}^n (y_i\\hat{n_i} - e^{\\hat{n_i}} - log(y_i!))\\)\nNull Deviance\n\\(D_{null} = -2 \\sum\\limits_{i=1}^n (y_i log(\\bar{y}) - \\hat{\\lambda_i} - log(y_i!))\\)\n\\(= -2 \\sum\\limits_{i=1}^n (y_i log(\\bar{y}) - \\bar{y} - log(y_i!))\\)\nSaturated Deviance\n\\(D_{saturated} = -2 \\sum\\limits_{i=1}^n (y_i log(y_i) - y_i - log(y_i!))\\)\nResidual Deviance\n\\(D_{residual} = D_{saturated} - D_{null}\\)\n\nDistributed with \\(\\chi^2\\)"
  },
  {
    "objectID": "glm.html#binomial-regression",
    "href": "glm.html#binomial-regression",
    "title": "Generalized Linear Models",
    "section": "Binomial Regression",
    "text": "Binomial Regression\n\\(\\frac{p}{1-p} = exp(\\beta_0 + \\beta_1 x)\\)\nLog Odds\n\\(log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1 x\\)\nIf we increase \\(x \\rightarrow x+1\\)\nThen \\(odds = exp(\\beta_0 + b_1(x+1)) = exp(\\beta_0 + \\beta_1x + \\beta_1) = exp(\\beta_1)exp(\\beta_0 + \\beta_1x)\\)\ni.e. we have been doing linear regression all along, but for the log-odds instead of probability.\n\n\\(\\eta = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 = log(\\frac{p}{1-p})\\)\n\n\\(\\beta_0\\) represents the log odds of success when all predictors are equal to 0\na unit increase in \\(x_j\\) with all other predictors held constant increases the odds of success by \\(e^{\\beta_j}\\)\n\n\n\\(sigm(\\beta_0 + \\beta_1x_1 + \\beta_2x_2) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2)}}\\)"
  },
  {
    "objectID": "glm.html#binomial-logistc-regression",
    "href": "glm.html#binomial-logistc-regression",
    "title": "Generalized Linear Models",
    "section": "Binomial / Logistc Regression",
    "text": "Binomial / Logistc Regression\n\nImport Libraries and Data\n\n\nCodelibrary(MASS)\nlibrary(tidyverse)\nlibrary(ISwR)\n\nWarning: package 'ISwR' was built under R version 4.3.3\n\nCodeadmission = read.csv(\"https://stats.idre.ucla.edu/stat/data/binary.csv\")\nhead(admission)\n\n  admit gre  gpa rank\n1     0 380 3.61    3\n2     1 660 3.67    3\n3     1 800 4.00    1\n4     1 640 3.19    4\n5     0 520 2.93    4\n6     1 760 3.00    2\n\n\n\nPerform Logistic Regression with admit as the response and rank as the categorical variable\n\n\nCodeadmission$rank &lt;- as.factor(admission$rank)\nadmission_glm &lt;- glm(admit ~ gre + gpa + rank, data = admission, family = binomial)\nsummary(admission_glm)\n\n\nCall:\nglm(formula = admit ~ gre + gpa + rank, family = binomial, data = admission)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.989979   1.139951  -3.500 0.000465 ***\ngre          0.002264   0.001094   2.070 0.038465 *  \ngpa          0.804038   0.331819   2.423 0.015388 *  \nrank2       -0.675443   0.316490  -2.134 0.032829 *  \nrank3       -1.340204   0.345306  -3.881 0.000104 ***\nrank4       -1.551464   0.417832  -3.713 0.000205 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 499.98  on 399  degrees of freedom\nResidual deviance: 458.52  on 394  degrees of freedom\nAIC: 470.52\n\nNumber of Fisher Scoring iterations: 4\n\nCodeconfint.default(admission_glm)\n\n                    2.5 %       97.5 %\n(Intercept) -6.2242418514 -1.755716295\ngre          0.0001202298  0.004408622\ngpa          0.1536836760  1.454391423\nrank2       -1.2957512650 -0.055134591\nrank3       -2.0169920597 -0.663415773\nrank4       -2.3703986294 -0.732528724\n\nCodehead(model.matrix(admission_glm))\n\n  (Intercept) gre  gpa rank2 rank3 rank4\n1           1 380 3.61     0     1     0\n2           1 660 3.67     0     1     0\n3           1 800 4.00     0     0     0\n4           1 640 3.19     0     0     1\n5           1 520 2.93     0     0     1\n6           1 760 3.00     1     0     0\n\n\n\nConstruct Reduced Model without rank. Conduct the likelihood ratio test to decide whether the reduced model is sufficient.\n\n\nCodeadmission_red_glm &lt;- glm(admit ~ gre + gpa, data = admission, family = binomial)\nsummary(admission_red_glm)\n\n\nCall:\nglm(formula = admit ~ gre + gpa, family = binomial, data = admission)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.949378   1.075093  -4.604 4.15e-06 ***\ngre          0.002691   0.001057   2.544   0.0109 *  \ngpa          0.754687   0.319586   2.361   0.0182 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 499.98  on 399  degrees of freedom\nResidual deviance: 480.34  on 397  degrees of freedom\nAIC: 486.34\n\nNumber of Fisher Scoring iterations: 4\n\nCodeanova(admission_red_glm, admission_glm, test = \"Chisq\") # likelihood ratio test\n\nAnalysis of Deviance Table\n\nModel 1: admit ~ gre + gpa\nModel 2: admit ~ gre + gpa + rank\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       397     480.34                          \n2       394     458.52  3   21.826 7.088e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe test reveals a small p-value, therefore we can reject the null hypothesis in favor of the alternative hypothesis that the reduced model is not sufficient.\n\n\n\\(H_0\\): the reduced model is sufficient\n\n\\(H_a\\): the reduced model is not sufficient"
  },
  {
    "objectID": "glm.html#poisson-regression",
    "href": "glm.html#poisson-regression",
    "title": "Generalized Linear Models",
    "section": "Poisson Regression",
    "text": "Poisson Regression\n\nDataset for Poisson Regression\n\n\nCodegala = read.table(\"https://www.colorado.edu/amath/sites/default/files/attached-files/gala.txt\", header = TRUE, sep = \"\\t\")\ngala = gala[,-2]\n\nhead(gala)\n\n             Species  Area Elevation Nearest Scruz Adjacent\nBaltra            58 25.09       346     0.6   0.6     1.84\nBartolome         31  1.24       109     0.6  26.3   572.33\nCaldwell           3  0.21       114     2.8  58.7     0.78\nChampion          25  0.10        46     1.9  47.4     0.18\nCoamano            2  0.05        77     1.9   1.9   903.82\nDaphne.Major      18  0.34       119     8.0   8.0     1.84\n\nCodedim(gala)\n\n[1] 30  6\n\n\n\nSee how a linear model looks\n\n\nCodelmod = lm(Species ~ ., data = gala)\nsummary(lmod)\n\n\nCall:\nlm(formula = Species ~ ., data = gala)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-111.679  -34.898   -7.862   33.460  182.584 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.068221  19.154198   0.369 0.715351    \nArea        -0.023938   0.022422  -1.068 0.296318    \nElevation    0.319465   0.053663   5.953 3.82e-06 ***\nNearest      0.009144   1.054136   0.009 0.993151    \nScruz       -0.240524   0.215402  -1.117 0.275208    \nAdjacent    -0.074805   0.017700  -4.226 0.000297 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 60.98 on 24 degrees of freedom\nMultiple R-squared:  0.7658,    Adjusted R-squared:  0.7171 \nF-statistic:  15.7 on 5 and 24 DF,  p-value: 6.838e-07\n\nCodepar(mfrow = c(2,2))\nplot(lmod)\n\nWarning in sqrt(crit * p * (1 - hh)/hh): NaNs produced\n\nWarning in sqrt(crit * p * (1 - hh)/hh): NaNs produced\n\n\n\n\n\n\n\nCodedf = data.frame(x = fitted(lmod), y = stdres(lmod))\nggplot(df, aes(x = x, y = y)) + \n    geom_point() + \n    theme_bw() + \n    geom_hline(yintercept = 0)\n\n\n\n\n\n\n\n\nPoisson Regression\n\n\nCodeglmod = glm(Species ~ ., data = gala, family = poisson)\nsummary(glmod)\n\n\nCall:\nglm(formula = Species ~ ., family = poisson, data = gala)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.155e+00  5.175e-02  60.963  &lt; 2e-16 ***\nArea        -5.799e-04  2.627e-05 -22.074  &lt; 2e-16 ***\nElevation    3.541e-03  8.741e-05  40.507  &lt; 2e-16 ***\nNearest      8.826e-03  1.821e-03   4.846 1.26e-06 ***\nScruz       -5.709e-03  6.256e-04  -9.126  &lt; 2e-16 ***\nAdjacent    -6.630e-04  2.933e-05 -22.608  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3510.73  on 29  degrees of freedom\nResidual deviance:  716.85  on 24  degrees of freedom\nAIC: 889.68\n\nNumber of Fisher Scoring iterations: 5\n\nCodepar(mfrow = c(2,2)); plot(glmod)\n\nWarning in sqrt(crit * p * (1 - hh)/hh): NaNs produced\n\nWarning in sqrt(crit * p * (1 - hh)/hh): NaNs produced\n\n\n\n\n\n\n\nCodedf = data.frame(x = predict(glmod, type = \"link\"), y = stdres(glmod))\nggplot(df, aes(x = x, y = y)) + \n    geom_point() + \n    theme_bw() + \n    geom_hline(yintercept = 0)\n\n\n\n\n\n\n\n\nInterpret the parameter associated with Nearest\n\n\nCodeexp(8.826e-03)\n\n[1] 1.008865\n\n\nThis means that, given the model is correct, a one unit increase in Nearest is associated with a multiplicitive increase of \\(e^{8.826e-03} = 1.01\\) in species, on average, adjusting for other predictors.\n\nCalculate the deviance for Poisson regression. Does this value show in the summary? Also, check the goodness of fit of this model using Pearson’s \\(\\chi^2\\) statistic. What can you conclude about the fit?\n\n\nCoded_res &lt;- with(gala,\n              -2*sum(Species*log(fitted(glmod)/Species) - (Species - fitted(glmod))))\nd_res\n\n[1] 716.8458\n\nCodechisq_test &lt;- with(gala, sum((Species - fitted(glmod))^2/fitted(glmod)))\n\npval &lt;- 1 - pchisq(chisq_test, 24)\npval\n\n[1] 0\n\nCodesummary(glmod)\n\n\nCall:\nglm(formula = Species ~ ., family = poisson, data = gala)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.155e+00  5.175e-02  60.963  &lt; 2e-16 ***\nArea        -5.799e-04  2.627e-05 -22.074  &lt; 2e-16 ***\nElevation    3.541e-03  8.741e-05  40.507  &lt; 2e-16 ***\nNearest      8.826e-03  1.821e-03   4.846 1.26e-06 ***\nScruz       -5.709e-03  6.256e-04  -9.126  &lt; 2e-16 ***\nAdjacent    -6.630e-04  2.933e-05 -22.608  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3510.73  on 29  degrees of freedom\nResidual deviance:  716.85  on 24  degrees of freedom\nAIC: 889.68\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe Chisq statistic is very large, and the p-value is small, so we would reject the null hypothesis that the model fits the data."
  },
  {
    "objectID": "glm.html#what-do-prediction-look-like",
    "href": "glm.html#what-do-prediction-look-like",
    "title": "Generalized Linear Models",
    "section": "What do Prediction Look Like?",
    "text": "What do Prediction Look Like?\nLet’s look at the binomial regression model for this.\n\nCode# predict admit from gre, gpa, rank\nnewData &lt;- data.frame(gre=600, gpa=3.8, rank=as.factor(4))\npredict(admission_glm, newdata = newData)\n\n        1 \n-1.127445 \n\nCodepredict(admission_glm, newdata = newData, type = 'response')\n\n       1 \n0.244633 \n\n\n\nNote that type response returns the percent associated with the model.\n\nSide Note: type parameters in predict\n\nresponse: default for many model types and returns the predicted values on the scale of the response variable\n\nclass: often used with classification models, this returns the predicted class label\nterms: this returns the contribution of each term in the linear predictor to prediction\nlink: in GLMs, this returns the linear predictors, i.e. the predicted values on the scale of the linear predictor (before applying the link function)\nprob: some models allow for returning a matrix of class probabilities, where each column corresponds to a class and each row corresponds to an observation\nvotes: for ensemble models, this might return a matrix of vote counts from different models"
  },
  {
    "objectID": "linear_regression.html",
    "href": "linear_regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "This page features information about creating and interpreting regression models."
  },
  {
    "objectID": "linear_regression.html#the-expected-value-of-the-response-variable",
    "href": "linear_regression.html#the-expected-value-of-the-response-variable",
    "title": "Linear Regression",
    "section": "The Expected Value of the Response Variable",
    "text": "The Expected Value of the Response Variable\nGiven \\(Y = \\alpha + \\beta X + \\epsilon\\), what is \\(E[Y]\\)?\n\\(E[Y] = E[\\alpha + \\beta X + \\epsilon]\\)\n\\(= E[\\alpha] + E[\\beta X] + E[\\epsilon]\\)\n\\(= \\alpha + \\beta E[X] + 0\\)\n\\(= \\alpha + \\beta X\\)"
  },
  {
    "objectID": "linear_regression.html#interpreting-parameters",
    "href": "linear_regression.html#interpreting-parameters",
    "title": "Linear Regression",
    "section": "Interpreting Parameters",
    "text": "Interpreting Parameters\n\\(\\beta\\): the slope of our true regression line represents the increase in our response from a unit increase in our feature."
  },
  {
    "objectID": "linear_regression.html#minimizing-the-residuals",
    "href": "linear_regression.html#minimizing-the-residuals",
    "title": "Linear Regression",
    "section": "Minimizing the Residuals",
    "text": "Minimizing the Residuals\nGiven our data, \\((x_1, y_1), \\dots, (x_n, y_n)\\) how do we minimize the residuals, \\(\\epsilon_i = y_i - (\\alpha + \\beta x_i)\\)? In other words, how do we minimize the sum of the squared errors?\n\ninsert image of vertical line from regression line to point, label it e_i\n\nGiven:\n\n\n\\(y_i\\): the actual value of the data point\n\n\\(\\hat{y_i}\\): the predicted value of the \\(i^{th}\\) data point\n\n\\(SSE = \\sum\\limits_{i=1}^{n} (y_i - \\hat{y_i})^2\\)\n\nthe point-estimates (single value estimated from the data) of the slope and intercept parameters are called the least-squares estimates, and are defined to be the values that minimize the SSE. The SSE can be thought of as a measure of how much variation in \\(Y\\) is left unexplained by the model."
  },
  {
    "objectID": "linear_regression.html#how-we-find-the-parameter-estimates",
    "href": "linear_regression.html#how-we-find-the-parameter-estimates",
    "title": "Linear Regression",
    "section": "How we Find the Parameter Estimates?",
    "text": "How we Find the Parameter Estimates?\n\\(SSE = \\sum\\limits_{i=1}^{n} (y_i - \\hat{y_i})^2\\)\n\\(= \\sum\\limits_{i=1}^{n} (y_i - (\\alpha - \\beta x_i))^2\\)\nAfter making this substitution, compute…\n\n\\(\\frac{\\partial SSE}{\\partial \\alpha} = 0\\)\n\\(\\frac{\\partial SSE}{\\partial \\beta} = 0\\)\n\nWhich yield the following solutions, respectively…\n\n\\(\\alpha = \\bar{y} - \\beta \\bar{x}\\)\n\\(\\beta = \\frac{\\bar{xy} - \\bar{x} \\bar{y}}{\\bar{x^2} - (\\bar{x})^2}\\)"
  },
  {
    "objectID": "linear_regression.html#estimating-the-variance",
    "href": "linear_regression.html#estimating-the-variance",
    "title": "Linear Regression",
    "section": "Estimating the Variance",
    "text": "Estimating the Variance\nGiven that the parameter \\(\\sigma^2\\) determines the spread of the data about the true regression line, our estimate of the variance is \\(\\hat{\\sigma}^2\\), which is equivalent to:\n\\(\\hat{\\sigma}^2 = \\frac{SSE}{n-2}\\)"
  },
  {
    "objectID": "linear_regression.html#coefficient-of-determination",
    "href": "linear_regression.html#coefficient-of-determination",
    "title": "Linear Regression",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\nThe coefficient of determination, \\(R^2\\), quantifies how well the model explains the data. \\(R^2\\) ranges from \\(0\\) to \\(1\\).\nThe regression sum of squares is given by:\n\\(SSR = \\sum\\limits_{i=1}^n (\\hat{y_i} - \\bar{y})^2\\)\nand gives a sense of how much variation in \\(Y\\) is explained by our model.\nA quantitative measure of the total amount of variation in observed \\(Y\\) values is given by the total sum of squares:\n\\(SST = \\sum\\limits_{i=1}^n (y_i - \\bar{y})^2\\)\nSST is what we would get if we used the mean of the data as our model.\nNote that the sum of squared deviations about the least-squares line is smaller than the sum of squared deviations about nay other line.\n\\(SSE \\leq SST\\)\n\nInclude image of the three measures.\n\n\\(R^2\\)\nTherefore, the ratio of \\(\\frac{SSE}{SST}\\) is the proportion of the total variation in the data (SST) that cannot be explained by the SLR model (SSE). So we define the coefficient of determination \\(R^2\\) to be the proportion of variance that can be explained by the model.\n\\(R^2\\) has a magnitude between \\(0\\) and \\(1\\), with the closer to \\(1\\) be the better (i.e. the higher the number the more of the variation that can be explained by the model).\n\\(R^2 = 1 - \\frac{SSE}{SST}\\)"
  },
  {
    "objectID": "linear_regression.html#slope-distribution",
    "href": "linear_regression.html#slope-distribution",
    "title": "Linear Regression",
    "section": "Slope Distribution",
    "text": "Slope Distribution\nDistribution\n\\(\\hat{\\beta} \\sim N(\\beta, \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2})\\)\n\\(SE(\\hat{\\beta}) = \\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}}\\)\nHypothesis Tests\n\\(H_0\\): \\(\\beta = 0\\)\n\\(H_A\\): \\(\\beta \\neq 0\\)\nConfidence Intervals\n\\(\\hat{\\beta} \\pm t_{\\alpha / 2, df = n-2} SE(\\hat{\\beta})\\)\n\nNote the statistic from the confidence interval, we use t-tests for the coefficients (i.e. individul coefficients for linear regression models). Specifically, used in finding if a feature has an effect on the response. In the case of a small enough p-value, we can reject the null hypothesis in favor of the alternative hypothesis which means there is statistical evidence that the associated feature (variable) has an effect on the response variable."
  },
  {
    "objectID": "linear_regression.html#summary-of-slr",
    "href": "linear_regression.html#summary-of-slr",
    "title": "Linear Regression",
    "section": "SUMMARY OF SLR",
    "text": "SUMMARY OF SLR\nMain Formula\n\\(Y = \\alpha + \\beta X + \\epsilon\\)\nVariance & Slope Distribution (\\(\\beta\\)s)\n\\(\\hat{\\sigma}^2 = \\frac{SSE}{n-2}\\)\n\\(SE(\\hat{\\beta}) = \\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}}\\)\nCI for t-tests on \\(\\beta\\)\n\n\\(\\hat{\\beta} \\pm t_{\\alpha / 2, df = n-2} SE(\\hat{\\beta})\\)\nCoefficient of Determination\n\n\n\n\n\n\n\n\n\n\nSSE\nSSR\nSST\n\\(R^2\\)\n\n\n\nDescription\nMeasure of how much variation in \\(Y\\) is left unexplained by the model\nHow much variation in \\(Y\\) is explained by our model\nTotal amount of variation in observed \\(Y\\) values (what we would get if we used the mean of the data as our model)\nThe proportion of variance that can be explained by the model\n\n\nFormula\n\\(\\sum\\limits_{i=1}^{n} (y_i - \\hat{y_i})^2\\)\n\\(\\sum\\limits_{i=1}^n (\\hat{y_i} - \\bar{y})^2\\)\n\\(\\sum\\limits_{i=1}^n (y_i - \\bar{y})^2\\)\n\\(1 - \\frac{SSE}{SST}\\)\n\n\n\nThe closer to 1 \\(R^2\\) is, the better the model fits the data."
  },
  {
    "objectID": "linear_regression.html#workflow-for-simple-linear-regression",
    "href": "linear_regression.html#workflow-for-simple-linear-regression",
    "title": "Linear Regression",
    "section": "Workflow for Simple Linear Regression",
    "text": "Workflow for Simple Linear Regression\n\nPlot the data as a scatter plot\n\n\nDoes linearity seem appropriate?\nCompute and overlay best-fit line\n\n\nConsider assumptions in SLR\n\n\nPlot a histogram of the residuals (are they normal?): WANT NORMAL\n\nPlot the residuals against x (are they changing?): WANT RANDOM\n\n\n\nInclude Plots"
  },
  {
    "objectID": "linear_regression.html#simple-linear-regression-in-r",
    "href": "linear_regression.html#simple-linear-regression-in-r",
    "title": "Linear Regression",
    "section": "Simple Linear Regression in R",
    "text": "Simple Linear Regression in R\nThe following functions are paramount in implementing and interpreting simple linear regression in R:\n\nlm()\nsummary()\n\n\nLoad Library\n\n\nCode# import libraries\nlibrary(tidyverse)\n\n\n\nLoad Data (use txhousing from built in data)\n\n\nCodedf &lt;- txhousing\nhead(df)\n\n# A tibble: 6 × 9\n  city     year month sales   volume median listings inventory  date\n  &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Abilene  2000     1    72  5380000  71400      701       6.3 2000 \n2 Abilene  2000     2    98  6505000  58700      746       6.6 2000.\n3 Abilene  2000     3   130  9285000  58100      784       6.8 2000.\n4 Abilene  2000     4    98  9730000  68600      785       6.9 2000.\n5 Abilene  2000     5   141 10590000  67300      794       6.8 2000.\n6 Abilene  2000     6   156 13910000  66900      780       6.6 2000.\n\n\n\nCreate a simple linear regression model to see if we can explain and predict volume from listings\n\n\nCodeslr &lt;- lm(volume ~ listings, data = df)\nsummary(slr)\n\n\nCall:\nlm(formula = volume ~ listings, data = df)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-704067392  -27385680  -12084812    -951885 1686521981 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1501601.1  1750052.1  -0.858    0.391    \nlistings       36990.0      258.1 143.319   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 130500000 on 7174 degrees of freedom\n  (1426 observations deleted due to missingness)\nMultiple R-squared:  0.7411,    Adjusted R-squared:  0.7411 \nF-statistic: 2.054e+04 on 1 and 7174 DF,  p-value: &lt; 2.2e-16\n\n\nFrom this summary, we can see that listings is a statistically significant factor (low p-value) and that the \\(R^2\\) for the model is somewhat acceptable at \\(0.7411\\). Although the intercept being negative does raise some concern, this would mean that for a time period with \\(0\\) listings, we would get a negative volume. The model also says that for each new listing, we can expect the volume to increase by \\(36990\\)."
  },
  {
    "objectID": "linear_regression.html#matrix-algebra-applications-solutions",
    "href": "linear_regression.html#matrix-algebra-applications-solutions",
    "title": "Linear Regression",
    "section": "Matrix Algebra Applications & Solutions",
    "text": "Matrix Algebra Applications & Solutions\nIf we have a perfectly square \\(X\\) matrix, then we can solve for \\(\\beta\\) via:\n\\(Y = X \\beta \\rightarrow X^{-1} Y = \\beta\\).\nHowever, we can almost guarantee that \\(X\\) will not be square. Look at this following linear algebra application:\n\nGiven a matrix \\(A\\) of any dimensions, both \\(A^TA\\) and \\(AA^T\\) will result in square matrices.\n\nTherefore, given \\(Y = X \\beta\\), we can solve via the following:\n\\((X^TX)^{-1}X^TY = \\beta\\)"
  },
  {
    "objectID": "linear_regression.html#interpreting-mlr",
    "href": "linear_regression.html#interpreting-mlr",
    "title": "Linear Regression",
    "section": "Interpreting MLR",
    "text": "Interpreting MLR\nGiven \\(y_i = \\beta_0 + B_1 x_{i, 1} + \\dots + B_p x_{i, p} + \\epsilon_i\\), parameter \\(\\beta_k\\) is the expected change in the response associated with a unit change in the value of feature \\(x_k\\) while all of the other features are held fixed."
  },
  {
    "objectID": "linear_regression.html#quantifying-goodness-of-fit",
    "href": "linear_regression.html#quantifying-goodness-of-fit",
    "title": "Linear Regression",
    "section": "Quantifying Goodness of Fit",
    "text": "Quantifying Goodness of Fit\n\n\\(R^2\\) vs. \\(R_a^2\\)\n\nLike in SLR, we can also calculate measures like SSE, SST, and \\(R^2\\) for MLR.\n\n\\(SSE = \\sum\\limits_{i=1}^n (y_i - \\hat{y_i})^2 = \\sum\\limits_{i=1}^n (y_i - (\\beta_0 + B_1 x_{i, 1} + \\dots + B_p x_{i, p}))^2\\)\n\\(SST = \\sum\\limits_{i=1}^n (y_i - \\bar{y})^2\\)\n\\(R^2 = 1 - \\frac{SSE}{SST}\\)\n\nAlthough we can calculate \\(R^2\\) for an MLR model, we run into the issue of multiple comparisons. The Adjusted \\(R^2\\), \\(R_a^2\\), is a better indicator of goodness of fit for MLR models. The adjusted version penalizes for having too many features that are not reducing SSE.\n\\(R_a^2 = 1 - \\frac{SSE/(n-p-1)}{SST/(n-1)}\\)\nCovariance & Correlation\nWe can discover relationships among features by performing a correlation analysis. If the value of one feature changes, how will this affect the other features.\nTraditionally, these measurements are calculated via:\n\nCovariance: \\(Cov(X, Y) = E[(X - E[X])(Y -E[Y])]\\)\n\nCorrelation (Pearson’s): \\(\\rho(X, Y) = \\frac{Cov(X, Y)}{\\sqrt{Var(X) Var(Y)}}\\)\n\n\nHowever, in an MLR, we estimate these relationships using formulas analogous to the sample variance.\n\nSample Covariance: \\(S^2_{XY} = \\frac{1}{n-1} \\sum\\limits_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\)\n\nSample Correlation \\(\\hat{\\rho}(X &lt; Y) = \\frac{S^2_{XY}}{\\sqrt{S^2_{X}S^2_{Y}}}\\)\n\n\nwhere \\(S^2_{X}\\) and \\(S^2_{Y}\\) are the variances for \\(X\\) and \\(Y\\), respectively."
  },
  {
    "objectID": "linear_regression.html#individual-t-tests",
    "href": "linear_regression.html#individual-t-tests",
    "title": "Linear Regression",
    "section": "Individual t-tests",
    "text": "Individual t-tests\nSuppose we want to test:\n\n\n\\(H_0\\): \\(\\beta_j = c\\)\n\n\n\\(H_A\\): \\(\\beta_j \\neq c\\)\n\n\nfor some \\(c \\in \\mathbb{R}\\).\nWe would use a t-test with the test statistic:\n\\(t_{stat} = \\frac{\\hat{\\beta_j} - c}{SE(\\hat{\\beta_j})}\\)\nRecall our CI for \\(\\beta\\):\nCI: \\(\\hat{\\beta} \\pm t_{\\alpha/2, df = n - 2} \\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2}}\\)\n\\(= \\hat{\\beta} \\pm t_{\\alpha/2, df = n - 2} \\hat{SE(\\hat{\\beta_j})}\\)\nCompare the test statistic with the t-test critical value (\\(t_{\\alpha/2, df = n - 2}\\)) and proceed from there.\nMLR in R: Individual t-test Output from summary\n\n\nFrom same dataset used in SLR, create MLR. Note: we can specify all variables like y ~ x1 + x2 + ... + xn, or we can call all of the variables besides the response variable by using ..\n\n\nCodemlr &lt;- lm(volume ~ month + sales + listings, data = df)\nsummary(mlr)\n\n\nCall:\nlm(formula = volume ~ month + sales + listings, data = df)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-425617453   -8326819    1919907    8285352  451201576 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -7064965.5  1055382.2  -6.694 2.33e-11 ***\nmonth        -103150.3   141167.5  -0.731    0.465    \nsales         274509.7     1078.4 254.559  &lt; 2e-16 ***\nlistings      -12218.7      209.8 -58.251  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41190000 on 7172 degrees of freedom\n  (1426 observations deleted due to missingness)\nMultiple R-squared:  0.9742,    Adjusted R-squared:  0.9742 \nF-statistic: 9.035e+04 on 3 and 7172 DF,  p-value: &lt; 2.2e-16\n\n\nIn the above summary printout, note that:\n\nStd. Error = \\(SE(\\hat{\\beta})\\)\n\nt value: \\(t_{stat}\\)\n\nPr(&gt;|t|): p-value\n\nNote that the lm() and summary functions asume a hypothesis test of:\n\n\n\\(H_0\\): \\(\\beta_j = 0\\)\n\n\n\\(H_A\\): \\(\\beta_j \\neq 0\\)\n\n\nSo, depending on the significance level we had set beforehand, we either reject the null hypothesis in favor of the alternative hypothesis or fail to reject the null hypothesis. In the case of a small enough p-value, we can reject the null hypothesis in favor of the alternative hypothesis which means there is statistical evidence that the associated feature (variable) has an effect on the response variable.\nType I Errors in Multiple t-tests\n\nHowever, it is not a good idea to conduct several t-tests at a time!\n\nExample: Suppose that we conduct 10 hypothesis tests at significance level \\(\\alpha = 0.05\\). So, the probability of type 1 error is 0.05 for any individual test.\nWhat is the probability of a type I error ocurring in at least one of these 10 tests?\n\\(P(\\text{at least 1 type I error}) = 1 - P(\\text{no type I error})\\)\n\\(= 1 - (0.95)^{10}\\)\n\\(\\approx 0.40\\)\nWhich is high!"
  },
  {
    "objectID": "linear_regression.html#multicollinearity-collinearity-non-identifiability",
    "href": "linear_regression.html#multicollinearity-collinearity-non-identifiability",
    "title": "Linear Regression",
    "section": "Multicollinearity / Collinearity / Non-Identifiability",
    "text": "Multicollinearity / Collinearity / Non-Identifiability\nMulticollinearity, also known non-identifiability, can occur when there is linear dependence between columns of data. “Strict” non-identifiability occurs with true linear dependence and can be diagnosed with NA rows for MLR coefficients in a given summary, is rare. “Near” non-identifiability is less rare but can be tricky to diagnose. One method of diagnosing “near” collinearity is to recognize “fishy” coefficients, such as negatives when expecting positive.\nThis is a concern because linear dependence causes non-invertibility in the matrices \\(X^TX\\) and \\(XX^T\\) for any given matrix \\(X\\). This is disruptive in creating MLR models."
  },
  {
    "objectID": "linear_regression.html#f-test",
    "href": "linear_regression.html#f-test",
    "title": "Linear Regression",
    "section": "F-test",
    "text": "F-test\nHow do control the overall type I error rate when we need to test whether to drop several predictors from a model. We can use a simultaneous test, the F-test.\nConsider two different models from a dataset with \\(p\\) features:\n\nFull Model: \\(\\Omega\\): \\(Y = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\\)\n\nReduced Model: \\(\\omega\\): \\(Y = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_q X_q\\), where \\(q &lt; p\\)\n\n\nWe can consider testing the following hypothesis:\n\n\n\\(H_0\\): \\(\\omega\\) is sufficient\n\n\\(H_A\\): \\(\\omega\\) is not sufficient\n\nThe Full F-test\nThe null hypothesis is the most reduced model possible\nThe idea:\n\n\n\\(H_0\\): \\(y_i = \\beta_0 + \\epsilon_i\\)\n\n\n\\(H_A\\): at least 1 \\(B_j \\neq 0\\), where \\(j = 1, \\dots, p\\)\n\n\nIn other words, we want to check whether ALL coefficients are 0:\n\n\n\\(H_0\\): \\(\\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\\)\n\n\n\\(H_A\\): \\(B_k \\neq 0\\) for at least one value of \\(k = 1, \\dots, p\\)\n\n\n\nThe null hypothesis in the MLR case says that there is no useful linear relationship between \\(y\\) and any of the \\(p\\) predictors.\n\nThe null hypothesis essentially states that there is no useful linear relationship between the response and any of the predictors. A small enough p-value suggests strong evidence against the null hypothesis, or in other words the model is better than the most reduced model possible. Useful in multiple linear regression (MLR) where the individual t-tests are suggesting evidence against the null hypothesis may result in type I errors.\n\\(F_{stat} = \\frac{\\frac{SST - SSE}{p}}{\\frac{SSE}{n-p-1}}\\)\nwhere \\(p = df_{SST} - df_{SSE}\\)\nThe \\(F_{stat}\\) is a measure of how much better our model is than just using the mean.\n\nIf \\(H_0\\) were true, then we would expect to see \\(F_{stat} \\approx 1\\). In other words, if we see an \\(F_{stat}\\) around 1, we will likely fail to reject the null hypothesis.\n\n\nIf \\(H_A\\) were true, then \\(SSE &lt; SST \\rightarrow F_{stat} &gt;&gt; 1\\)\n\nThe Partial F-test\nSuppose we have the following setup:\n\nFull Model: \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4\\)\n\nReduced Model: \\(y = \\beta_0 + \\beta_2 x_2 + \\beta_4 x_4\\)\n\n\nAre the missing features important, or are we okay with the reduced model?\nHypothesis Test for the Partial F-test:\n\n\n\\(H_0\\): \\(\\beta_1 = \\beta_3 = 0\\)\n\n\n\\(H_A\\): at least one of \\(\\beta_1\\), \\(\\beta_3\\) \\(\\neq 0\\)\n\n\nStrategy: fit the full and reduced models, then determine if the difference in performance is real or just due to chance.\nLet’s examine some measurements from the full and reduced models.\n\n\n\\(SSE_{full}\\): variation unexplained by the full model\n\n\\(SSE_{reduced}\\): variation unexplained by the reduced model\n\nIntuitively, if \\(SSE_{full}\\) is much smaller than \\(SSE_{reduced}\\), the full model fits the data much better than the reduced model. Therefore, the appropriate test statistic should depend on the difference between the unexplained variation.\nGive a full model with \\(p\\) features and a reduced model with \\(k\\) features:\n\\(F_{stat-partial} = \\frac{(SSE_{reduced} - SSE_{full}) / p - k}{SSE_{full} / (n-p-1)} \\sim F_{p-k, n-p-1}\\)\nwith the rejection region: \\(F \\geq F_{\\alpha, p-k, n-p-1}\\)\nNote that \\(p-k\\) comes from the differences in degrees of freedom:\n\\(df_{reduced} - df_{full} = (n-k-1) - (n-p-1) = p - k\\)\n\nLarge \\(F_{stat} \\rightarrow\\) Low P-Value from F-distribution \\(\\rightarrow\\) Well Fitting Model\n\nPrincipal of Parsimony: The principle that the most acceptable explanation of an occurrence, phenomenon, or event is the simplest, involving the fewest entities, assumptions, or changes.\nWe’re striving to create an MLR model which explains the variation in the data with relatively few features that are easily interpreted."
  },
  {
    "objectID": "linear_regression.html#feature-selection",
    "href": "linear_regression.html#feature-selection",
    "title": "Linear Regression",
    "section": "Feature Selection",
    "text": "Feature Selection\nThe number of models to test can exponentially increase with the number of features.\nGiven \\(p\\) features, there are \\(\\sum\\limits_{i=0}^p = 2^p\\) possible models.\nWhat are some ways we can reduce the impossible amount of models to try?\nForward Selection\nA greedy algorithm for adding features.\n\nFit a null model with an intercept but no slopes\nFit p individual SLR models - one for each possible feature. Add to the null model the one that improves the performance the most, based on some measure (i.e. decreases SSE the most, increases F-statistic the most, etc.)\nFit (p - 1) MLR models - one for each of the remaining features along with the feature we isolated from step 2. Add the one that improves model performance the most.\nRepeat until some stopping criterion is reached (i.e. some threshold SSE, or some fixed number of features, etc.)\nBackward Selection\nA greedy algorithm for removing features.\n\nFit model with available features.\nRemove the feature with the largest p-value (i.e. the least significant feature).\nRepeat until some stopping criterion is reached (i.e. some threshold SSE, or some fixed number of features, etc.)."
  },
  {
    "objectID": "linear_regression.html#basic-interpretations",
    "href": "linear_regression.html#basic-interpretations",
    "title": "Linear Regression",
    "section": "Basic Interpretations",
    "text": "Basic Interpretations\nLet’s recreate our MLR from earlier:\n\nCodemlr &lt;- lm(volume ~ month + sales + listings, data = df)\n\n\nsummary\n\nCodesummary(mlr)\n\n\nCall:\nlm(formula = volume ~ month + sales + listings, data = df)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-425617453   -8326819    1919907    8285352  451201576 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -7064965.5  1055382.2  -6.694 2.33e-11 ***\nmonth        -103150.3   141167.5  -0.731    0.465    \nsales         274509.7     1078.4 254.559  &lt; 2e-16 ***\nlistings      -12218.7      209.8 -58.251  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41190000 on 7172 degrees of freedom\n  (1426 observations deleted due to missingness)\nMultiple R-squared:  0.9742,    Adjusted R-squared:  0.9742 \nF-statistic: 9.035e+04 on 3 and 7172 DF,  p-value: &lt; 2.2e-16\n\n\nanova\n\nCodeanova(mlr)\n\nAnalysis of Variance Table\n\nResponse: volume\n            Df     Sum Sq    Mean Sq    F value    Pr(&gt;F)    \nmonth        1 1.5469e+17 1.5469e+17     91.194 &lt; 2.2e-16 ***\nsales        1 4.5387e+20 4.5387e+20 267563.442 &lt; 2.2e-16 ***\nlistings     1 5.7558e+18 5.7558e+18   3393.153 &lt; 2.2e-16 ***\nResiduals 7172 1.2166e+19 1.6963e+15                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe anova table gives us:\n\n\nDf: degrees of freedom\n\nSum Sq: SSE (for Residuals)\n\nSum Sq: SST (total column)\n\nMean Sq: SSR (for features)\n\nresiduals\n\nCodeggplot() +\n  geom_density(aes(residuals(mlr)))\n\n\n\n\n\n\n\n\nCodeggplot() +\n  geom_point(aes(x = seq(1, length(residuals(mlr))), y=residuals(mlr)))\n\n\n\n\n\n\n\nDon’t run, but this will return the predicted response values: fitted(mlr)"
  },
  {
    "objectID": "linear_regression.html#f-tests",
    "href": "linear_regression.html#f-tests",
    "title": "Linear Regression",
    "section": "F-tests",
    "text": "F-tests\nLet’s check out the F-tests.\n\nCodesummary(mlr)\n\n\nCall:\nlm(formula = volume ~ month + sales + listings, data = df)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-425617453   -8326819    1919907    8285352  451201576 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -7064965.5  1055382.2  -6.694 2.33e-11 ***\nmonth        -103150.3   141167.5  -0.731    0.465    \nsales         274509.7     1078.4 254.559  &lt; 2e-16 ***\nlistings      -12218.7      209.8 -58.251  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41190000 on 7172 degrees of freedom\n  (1426 observations deleted due to missingness)\nMultiple R-squared:  0.9742,    Adjusted R-squared:  0.9742 \nF-statistic: 9.035e+04 on 3 and 7172 DF,  p-value: &lt; 2.2e-16\n\n\nHere we notice that the individual t-tests are statistically significant, however be wary of type I errors. However, we can notice that the t-test associated with month is not significant, suggesting there is no evidence that parameter isn’t zero. Let’s check out the F-stat, which does have a high value with a low p-value associated with it. This is indicative of strong evidence that at least one of the \\(B_{k}\\)s are not \\(0\\).\nLet’s test a model removing month with a partial f-test.\n\nCodemlr_reduced &lt;- lm(volume ~ sales + listings, data = df)\n\n\n\nCodeanova(mlr_reduced, mlr)\n\nAnalysis of Variance Table\n\nModel 1: volume ~ sales + listings\nModel 2: volume ~ month + sales + listings\n  Res.Df        RSS Df  Sum of Sq      F Pr(&gt;F)\n1   7173 1.2167e+19                            \n2   7172 1.2166e+19  1 9.0568e+14 0.5339  0.465\n\n\nThe p-value for the f-test is very large, indicate that we don’t have enough evidence to reject the null hypothesis, which means that the reduced model is likely sufficient."
  },
  {
    "objectID": "review.html",
    "href": "review.html",
    "title": "Basic Review",
    "section": "",
    "text": "This page features review of basic statistical concepts and the linear algebra necessary for advanced statistics."
  },
  {
    "objectID": "review.html#discrete-random-variables",
    "href": "review.html#discrete-random-variables",
    "title": "Basic Review",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\nProbability Mass Function (pmf)\n\\(p(x) = P(X=x)\\)\n\n\\(p(x) \\geq 0\\)\n\\(\\sum_{x} p(x) = 1\\)\nCumulative Distribution Function (cdf)\n\\(F(x) = P(X \\leq x) = \\sum_{k \\leq x} p(k)\\)\n\n\\(0 \\leq F(x) \\leq 1\\)\nIf \\(a \\leq b \\rightarrow F(a) \\leq F(b)\\)"
  },
  {
    "objectID": "review.html#continuous-random-variables",
    "href": "review.html#continuous-random-variables",
    "title": "Basic Review",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\nProbability Density Function (pdf)\nIf X is a continuous random variable, then \\(f(x)\\) is a pdf if\n\n\\(P(a \\leq X \\leq b) = \\int_a^b f(x)dx\\)\n\\(f(x) \\geq 0\\)\n\\(\\int_{-\\infty}^\\infty f(x)dx = 1\\)\nCumulative Distribution Function (cdf)\nProperties of the CDF for a continuous RV X:\n\n\\(P(X \\leq x) = F(x)\\)\n\\(F(x) \\geq 0\\)\n\\(\\lim_{x \\rightarrow \\infty} F(x) = 1\\)\n\\(\\lim_{x \\rightarrow -\\infty} F(x) = 0\\)\nPDF to CDF, and Back Again\nBy the fundamental theorem of calculus, if \\(f(x)\\) is the pdf of a continuous variable X, and \\(F(x)\\) is the cdf the continuous variable X, then:\n\n\\(\\frac{d}{dx}F(x) = f(x)\\)\n\\(F(x) = \\int_{-\\infty}^x f(t)dt\\)"
  },
  {
    "objectID": "review.html#conditional-probability",
    "href": "review.html#conditional-probability",
    "title": "Basic Review",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\\(P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\\)\n\\(\\rightarrow P(B \\mid A) = \\frac{P(A \\mid B)P(B)}{P(A)}\\)\nIn the reference to the above formulas, the following definitions apply:\n\n\n\\(P(B \\mid A)\\): Posterior Probability\n\n\\(P(B)\\): Prior Probability\n\n\\(P(A)\\): Evidence\n\nNote that when dealing with more than two events, due to the law of total probability, the following is true:\n\\(P(A) = \\sum_{i = 1}^{n} P(A \\mid B_i)P(B_i)\\)\nIn other words, this applies:\n\\(P(B \\mid A) = \\frac{P(A \\mid B)P(B)}{P(A)} = \\frac{P(A \\mid B_j)P(B_j)}{\\sum_{i = 1}^{n} P(A \\mid B_i)P(B_i)}\\)"
  },
  {
    "objectID": "review.html#independence",
    "href": "review.html#independence",
    "title": "Basic Review",
    "section": "Independence",
    "text": "Independence\nGiven the events \\(A\\) and \\(B\\), the events are considered independent if\n\\(P(A \\cap B) = P(A)P(B)\\).\nThis has implications to the conditional probability definitions above."
  },
  {
    "objectID": "review.html#expected-value",
    "href": "review.html#expected-value",
    "title": "Basic Review",
    "section": "Expected Value",
    "text": "Expected Value\nExpected value, also known as expectation, mean, or average has slightly different methods of being calculated depending on if the random variable is discrete or continuous.\nDiscrete\n\\(E[X] = \\sum_{x}xp(x)\\)\n\\(E[g(X)] = \\sum_{x}g(x)p(x)\\)\nContinuous\n\\(E[X] = \\int_{-\\infty}^{\\infty} xf(x)dx\\)\n\\(E[g(X)] = \\int_{-\\infty}^{\\infty} g(x)f(x)dx\\)\nExpectation is a Linear Function\n\\(E[aX + b] = aE[X] + b\\)"
  },
  {
    "objectID": "review.html#variance-and-standard-deviation",
    "href": "review.html#variance-and-standard-deviation",
    "title": "Basic Review",
    "section": "Variance and Standard Deviation",
    "text": "Variance and Standard Deviation\nVariance, which essentially describes the spread of the data, particularly in reference to the mean, also has slightly different methods of being calculated depending on if the random variable is discrete or continuous.\nDiscrete\n\\(Var(X) = E[(X - E[X])]^2 = E[X^2] - (E[X])^2\\)\nContinuous\nNote: Notice from the formula above, all we really need to find are two different instances of expected value, namely\n\\(E[X^2]\\) and \\(E[X]\\)\nStandard Deviation\n\\(SD(X) = \\sqrt{Var(X)}\\)\nIndependence in Variance\nIf \\(X\\) and \\(Y\\) are independent, then\n\\(Var(X + Y) = Var(X) + Var(Y)\\)"
  },
  {
    "objectID": "review.html#percentiles-quantiles",
    "href": "review.html#percentiles-quantiles",
    "title": "Basic Review",
    "section": "Percentiles & Quantiles",
    "text": "Percentiles & Quantiles\nPercentiles\nFor a random variable \\(X\\), \\(x_p\\) is the \\(p^{th}\\) percentile of \\(X\\) if\n\\(P(X \\leq x_p) = p\\)\nFor example, the median is the \\(50^{th}\\) percentile.\nQuantiles\nWe often say percentile when given a percentage. The quantile is essentially just the decimal version of the percentile.\nFor example, the median is the \\(0.5\\) quantile.\nThe quantile function is the inverse of the CDF:\n\\(Q(q) = x\\) whenever \\(P(X \\leq x) = q\\)."
  },
  {
    "objectID": "review.html#symmetry",
    "href": "review.html#symmetry",
    "title": "Basic Review",
    "section": "Symmetry",
    "text": "Symmetry\nThe normal distribution is symmetric around its mean \\(\\mu\\). Hence, if we denote the normal distribution by \\(f_X(x)\\), then for any \\(c\\), \\(f_X(\\mu + c) = f_X(\\mu - c)\\).\nThis idea can be generalized for any distribution function. Suppose a distribution function, \\(f(x)\\), is symmetric around a point \\(x_0\\). Then for any \\(c\\), \\(f(x_0 + c) = f(x_0 - c)\\).\nIt can also be shown that if \\(X\\) is a continuous random variable whose ditribution is symmetric about \\(x_0\\), let’s use \\(x_0 = 0\\) for simplicity, then \\(E[2^{2k+1}] = 0\\), for any postive integer \\(k\\). That is, the expected value of any odd power of \\(X\\) is \\(0\\)."
  },
  {
    "objectID": "review.html#rule",
    "href": "review.html#rule",
    "title": "Basic Review",
    "section": "68-95-99.7 Rule",
    "text": "68-95-99.7 Rule\nGiven the random variable, \\(X \\sim N(0, 1)\\), we’ll examine the outcomes of when \\(X\\) is within 1, 2, and 3 standard deviations of the mean. We’ll build off the ideas of symmetry with symmetric ranges.\n\nSingle Standard Deviation From the Range\n\n\\(P(|x - \\mu| \\leq \\sigma)\\)\n\\(= 2P(x - \\mu \\leq \\sigma)\\) (just to show symmetry)\nOR\n\\(= P(-\\sigma \\leq x - \\mu \\leq \\sigma) = P(-1 \\leq \\frac{x-\\mu}{\\sigma} \\leq 1)\\)\n\\(= P(-1 \\leq Z \\leq 1)\\)\n\\(\\Phi(1) - \\Phi(-1)\\)\nNOTE: This does not depend on \\(\\mu\\) or \\(\\sigma\\).\nThe exact result of \\(P(|x - \\mu| \\leq \\sigma) \\approx 0.68\\)\n\nDouble Standard Deviation From the Range\n\n\\(P(|x - \\mu| \\leq 2\\sigma) \\approx 0.95\\)\n\nTriple Standard Deviation From the Range\n\n\\(P(|x - \\mu| \\leq 3\\sigma) \\approx 0.997\\)\nThis means that 99.7% of values for any normal distributions are within 3 standard deviations of the mean.\nThis rule is ONLY for normal distributions, but we can extend to other distributions."
  },
  {
    "objectID": "review.html#markovs-inequality",
    "href": "review.html#markovs-inequality",
    "title": "Basic Review",
    "section": "Markov’s Inequality",
    "text": "Markov’s Inequality\nGiven a random variable \\(X\\) such that \\(x \\geq 0\\) (this can be any distribution), then\n\\(P(X \\geq a) \\leq \\frac{E[X]}{a}\\)"
  },
  {
    "objectID": "review.html#chebyshevs-inequality",
    "href": "review.html#chebyshevs-inequality",
    "title": "Basic Review",
    "section": "Chebyshev’s Inequality",
    "text": "Chebyshev’s Inequality\nGiven a random variable \\(X\\) with finite variance, then\n\\(P(|X-\\mu|\\geq k\\sigma) \\leq \\frac{1}{k^2}\\)\nThis is saying that the probability of \\(X\\) being outside k standard deviations of mean is less than \\(\\frac{1}{k^2}.\\)\nNOTE: Different distributions (i.e. not normal distributions) are common in different domains. Markov’s and Chebyshev’s inequalities are great without knowing the exact distribution. In other words, they’re great for making general assumptions about general distributions."
  },
  {
    "objectID": "review.html#common-discrete-distributions",
    "href": "review.html#common-discrete-distributions",
    "title": "Basic Review",
    "section": "Common Discrete Distributions",
    "text": "Common Discrete Distributions\nBinomial Distribution\nIf a success has probability p, and there are n trials, what is the probability we have k successes:\n(Suppose a coin lands on heads with the probability p. If we flip the coin n times, what is the probability the coin lands on heads k times)\n\\(P(X=k) = \\binom{n}{k}p^k(1-p)^{n-k}\\)\n\n\\(E[X] = np\\)\n\\(Var(X) = np(1-p)\\)\nGeometric Distribution\nIf a success has probability p, what si the probability we k failures before the first success?\n\\(P(X = k) = p(1-p)^k\\)\n\n\\(E[X] = \\frac{1-p}{p}\\)\n\\(Var(X) = \\frac{1-p}{p^2}\\)\n\nDeriving E[X]\n\\(E[X] = \\sum_{k=0}^{\\infty} kp(1-p)^k\\)\n\\(= p(1-p)\\sum_{k=1}^{\\infty} kp(1-p)^{k-1}\\)\nNote that \\(kp(1-p)^{k-1}\\) is the definition of a derivative.\nSo, letting \\(q = 1-p\\), and using the fact that \\(\\sum \\frac{d}{dx}f(x) = \\frac{d}{dx}(\\sum f(x))\\),\n\\(E[X] = p(1-p)\\sum_{k=0}^{\\infty} (q^k)^{'} = p(1-p)(\\sum_{k=0}^{\\infty} q^k)^{'}\\)\nNote that \\(\\sum_{k=0}^{\\infty} q^k = \\frac{1}{1-q}\\) for \\(q \\leq 1\\)\n\\(= p(1-p)(\\frac{1}{1-q})^{'}\\)\n\\(= p(1-p)\\frac{1}{(1-q)^2}\\)\nSubstitute back \\(q = 1-p\\),\n\\(= \\frac{p(1-p)}{p^2} = \\frac{1-p}{p} = E[X]\\)\nDeriving Var(X)\n\\(Var(X) = E[X^2] - (E[X])^2\\)\nWe know \\(E[X]\\), so we only need to solve for \\(E[X^2]\\).\n\\(E[X^2] = \\sum_{k=0}^{\\infty} k^2p(1-p)^k = \\sum_{k=0}^{\\infty} k(k-1+1)p(1-p)^k\\)\n\\(= \\sum_{k=0}^{\\infty} k(k-1)p(1-p)^k + \\sum_{k=0}^{\\infty} kp(1-p)^k\\)\n\\(= p(1-p)^2 \\sum k(k-1)(1-p)^{k-2} + E[X]\\)\nNow just focusing on the left term, notice this is now the definition of a second derivative:\n\\(p(1-p)^2 \\sum ((1-p)^k)^{''}\\)\nPoisson Distribution\nSuppose we wanted to approximate the binomial distribution, \\(P(X=k) = \\binom{n}{k}p^k(1-p)^{n-k}\\), to find what it calculates on average for large numbers.\nSince \\(E[X] = np\\) (“on average”), let\n\\(\\lambda = np\\)\n\\(\\rightarrow p = \\frac{\\lambda}{n}\\)\nPlugging the \\(\\lambda\\) expression into the binomial distribution yields:\n\\(P(X=k) = \\binom{n}{k}(\\frac{\\lambda}{n})^k(1-(\\frac{\\lambda}{n}))^{n-k}\\)\nNow that we have an equation in terms of \\(\\lambda\\), what happens when we let \\(n \\rightarrow \\infty\\)?\n\\(\\binom{n}{k}(\\frac{\\lambda}{n})^k(1-(\\frac{\\lambda}{n}))^{n-k}\\)\n\\(= \\binom{n}{k}\\frac{(\\frac{\\lambda}{n})^k}{(1-\\frac{\\lambda}{n})^k}(1-\\frac{\\lambda}{n})^{n}\\)\nNote the following:\n\\(\\lim_{n \\rightarrow \\infty} (1-\\frac{\\lambda}{n})^{n} = e^{-\\lambda}\\)\nSo, just focusing on the left side of the equation:\n\\(\\binom{n}{k}\\frac{(\\frac{\\lambda}{n})^k}{(1-\\frac{\\lambda}{n})^k}\\)\n\\(= \\binom{n}{k}\\frac{\\lambda^k}{(n - \\lambda)^k}\\)\n\\(= \\frac{\\lambda^k}{k!}\\)\nTherefore,\n\\(\\lim_{n \\rightarrow \\infty} \\binom{n}{k}(\\frac{\\lambda}{n})^k(1-(\\frac{\\lambda}{n}))^{n-k} = \\frac{\\lambda^k}{k!} e^{-\\lambda}\\)\nTHEREFORE…\nFor large values of \\(n\\), the pmf for the binomial distribution is approximated by\n\\(\\frac{\\lambda^k}{k!} e^{-\\lambda}\\); where \\(\\lambda\\) is the average rate of success, and is known as the Poisson distribution. The Poisson distribution models the number of rare events in time or space.\nFor a more complete definition:\n\\(X \\sim Pois(\\lambda)\\) \\(P(X=k) = \\frac{\\lambda^k}{k!} e^{-\\lambda}\\) \\(E[X] = Var(X) = \\lambda\\)\nTo prove this is a probability mass function (pmf), we can use the Taylor’s Series:\n\\(\\sum_{k=0}^\\infty \\frac{\\lambda^k}{k!} = e^\\lambda\\)\n\\(\\sum_{k=0}^\\infty \\frac{\\lambda^k}{k!} e^{-\\lambda} = e^\\lambda \\sum_{k=0}^\\infty \\frac{\\lambda^k}{k!}\\)\n\\(= e^{-\\lambda}e^\\lambda = 1\\)\nRare: essentially means that two events cannot occur at once.\nUniform Distribution"
  },
  {
    "objectID": "review.html#common-continuous-distributions",
    "href": "review.html#common-continuous-distributions",
    "title": "Basic Review",
    "section": "Common Continuous Distributions",
    "text": "Common Continuous Distributions\nUniform Distribution\n\n\\(X \\sim Unif(a, b):\\)\n\n\\(f(x) = \\frac{a}{b-a}\\); \\(a \\leq x \\leq b\\)\n\ncdf: \\(P(X \\leq x) = F(x) = \\frac{x-a}{b-a}\\); \\(a \\leq x \\leq b\\)\n\n\n\\(E[X] = \\frac{a+b}{2}\\) middle of the interval\n\n\\(Var(X) = \\frac{(b-a)^2}{12}\\)\n\nCalculations:\ncdf:\n\\(F(x) = \\int_{-\\infty}^{x}f(t)dt = \\int_{-\\infty}^{x}f(t)dt + \\int_{a}^{x}f(t)dt\\) \\(= \\int_{a}^{x}\\frac{a}{b-a}dt = \\frac{x-a}{b-a}\\)\nE[X]:\n\\(E[X] = \\int_{-\\infty}^{\\infty} xf(x)dx\\) \\(= \\int_{a}^{b} x \\frac{1}{b-a} dx\\) \\(= \\frac{1}{2} x^2 \\frac{a}{b-a} \\mid_{a}^{b}\\)\n…simplifying…\n\\(= \\frac{a+b}{2}\\)\nVar(X):\n\\(Var(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2\\)\nWe only need to solve for \\(E[X^2]\\):\n\\(E[X^2] = \\int_{-\\infty}^{\\infty} x^2 \\frac{1}{b-a} dx\\) \\(= \\frac{1}{3} x^3 \\frac{a}{b-a} \\mid_{a}^{b}\\)\nPutting these together:\n\\(Var(X) = \\frac{(b-a)^2}{12}\\)\nNormal Distribution\nNormal Distribution: \\(\\sim N(\\mu, \\sigma)\\),\n\n\n\\(\\mu\\): expected value\n\nshifts the curve left or right\n\n\n\n\\(\\sigma\\): standard deviation (NOT variance)\n\nchanges width AND height\n\n\n\\(f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n\nStandard Normal Distribution:\nThe probability of the normal distribution, \\(P(a \\leq X \\leq B) = \\int\\limits_a^bf(x)dx\\), cannot be computed in closed form. only numerical integral can be used to find this. However, the Standard Normal Form provides normally quick computations through pre-computed values.\n\nThe Standard Normal Form: \\(Z \\sim N(0, 1)\\)\n\n\n\nClaim: \\(\\frac{X-\\mu}{\\sigma} \\sim N(0, 1)\\)\n\nLet \\(X\\) be a random variable, then \\(Z = \\frac{X-\\mu}{\\sigma}\\). We want to show that \\(E[Z] = 0\\) and \\(SD(Z) = 1\\)\nUsing the fact that expected value is linear:\n\\(E[\\frac{X-\\mu}{\\sigma}] = \\frac{E[X]-E[\\mu]}{E[\\sigma]} = \\frac{\\mu - \\mu}{E[\\sigma]} = 0\\)\nAside: \\(Var(aX) = a^2Var(X)\\)\n\\(Var(aX) = E[(aX)^2] - E[aX]^2\\)\n\\(= a^2E[X^2] - a^2E[X]^2 = a^2(E[X^2] - E[X]^2)\\)\n\\(= a^2Var(X)\\)\nUsing the above finding…\n\\(Var(Z) = Var(\\frac{X-\\mu}{\\sigma}) = \\frac{1}{\\sigma^2}Var(X-\\mu) = \\frac{1}{\\sigma^2}Var(X) = \\frac{\\sigma^2}{\\sigma^2} = 1\\)\n\\(Var(Z) = 1 \\rightarrow SD(Z) = \\sqrt{Var(Z)} = 1\\)\nWhat does the Z-score do?\nZ-score is defined as \\(Z = \\frac{x-\\mu}{\\sigma}\\), and measures how many standard deviations X is above (or below) the mean.\nThe Z-score calculates \\(P(Z \\leq x)\\), also commonly written as \\(\\Phi (X)\\), can be shifted to find other probabilities:\n\n\\(P(Z \\geq x) = 1 - P(Z \\leq x)\\)\n\\(P(a \\leq Z \\leq b) = P(Z \\leq b) - P(Z \\leq a)\\)\nExponential Distribution\n\nContinuous derivation of the Poisson Distribution\n\nRecall the Poisson distribution, which we used to model the number of rare events in time or space. The exponential distribution is the distribution of wait times in a Poisson process, i.e. how long until the next event.\n\n\n\\(F(X) = P(X \\leq x) = 1 - e^{-\\lambda x}\\), where \\(x&gt;0\\)\n\n\n\\(f(x) = \\lambda e^{-\\lambda x}\\), where \\(x&gt;0\\)\n\n\\(E[X] = \\frac{1}{\\lambda}\\)\n\\(Var(X) = \\frac{1}{\\lambda^2}\\)\n\n\\(X \\sim Exp(\\lambda)\\), where \\(\\lambda\\): the rate of occurrence of events\nMemoryless\n\nThe exponential distribution has the unique property of being memoryless, which can be illustrated with:\n\\(P(X &gt; t + s | X &gt; t) = P(X &gt; s)\\)\nTo put this result into words, it is saying “After waiting \\(t\\), the probability of an event happening in another \\(s\\) has the same probability of it happening in \\(s\\), if from \\(time = 0\\).”\nThis can be shown mathematically:\n\\(P(X &gt; t + s | X &gt; t) = \\frac{P(X &gt; t + s, X &gt; t)}{P(X &gt; t)}\\)\n\\(= \\frac{P(X &gt; t + s)}{P(X &gt; t)}\\)\n\\(= \\frac{1 - F(t+s)}{1 - F(t)}\\)\n\\(= \\frac{e^{-\\lambda(t+s)}}{e^{-\\lambda t}}\\)\n\\(= e^{-\\lambda s} = P(X &gt; s)\\)\n\nPoisson is “how many events in a time period”, while exponential is “what is the wait time between events”.\n\nAnother property of memorylessness is that:\n\\(E[X | X \\geq s] = s + E[X]\\)"
  },
  {
    "objectID": "review.html#central-limit-theorem",
    "href": "review.html#central-limit-theorem",
    "title": "Basic Review",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe idea behind the central limit theorem begins with the law of large numbers.\n\nLaw of Large Numbers\n\nAs the number of samples increases, the sample mean approaches the true mean:\nAs \\(n \\rightarrow \\infty\\), \\(\\bar{X_n} = \\frac{\\sum\\limits_x x_i}{n} \\rightarrow \\mu\\).\nAn application of Chebyshev’s Inequality provides the Weak LLN:\n\\(\\lim\\limits_{n \\rightarrow \\infty} P(|\\bar{X_n} - \\mu| &lt; \\epsilon) = 1\\)\n\nWeak Law of Large Numbers (WLLN)\n\nSpecifically, given\n\\(P(|\\bar{X_n} - \\mu| &gt; c) \\leq \\frac{\\sigma^2}{n * c^2}\\),\nThen taking the limit as \\(n \\rightarrow \\infty\\),\n\\(\\lim\\limits_{n \\to +\\infty} P(|\\bar{X}_n - \\mu| &gt; c) \\leq \\lim\\limits_{n \\to +\\infty} \\frac{\\sigma^2}{nc^2} = 0\\)\nDue to the axiom of probability, for any event \\(A\\), \\(P(A) \\geq 0\\), then we know that\n\\(0 \\leq (|\\bar{X}_n - \\mu| &gt; c) \\leq 0\\), then it follows that\n\\(\\lim\\limits_{n \\to +\\infty} P(|\\bar{X}_n - \\mu| &gt; c) = 0\\).\nIn conclusion, the WLLN says no matter how small we pick \\(c\\) to be, the probability of the sample mean being further away from the mean than that small number \\(c\\) goes to zero. Or, no matter how small we pick \\(c\\) to be, the probability of sample mean being with \\(c\\) of the true mean eventually approaches 1.\n\nThe Central Limit Theorem\n\nThis leads us to the Central Limit Theorem (CLT):\nAs \\(n \\rightarrow \\infty\\) for \\(\\bar{X} \\sim N(\\mu, \\frac{\\sigma}{\\sqrt{n}})\\),\nwhich holds regardless of what the underlying population distribution is.\nIn other words, the CLT tells us that the as the sample size \\(n\\) increases, the sample mean of \\(X\\) is close to normally distributed with expected value \\(\\mu\\) and standard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)."
  },
  {
    "objectID": "review.html#joint-variable-distributions",
    "href": "review.html#joint-variable-distributions",
    "title": "Basic Review",
    "section": "Joint Variable Distributions",
    "text": "Joint Variable Distributions\nGeneral Information\nCumulative Density Function: \\(F_{X,Y}(x, y) = P(X \\leq x, Y \\leq y)\\)\nDiscrete PMF: \\(p(x, y) = P(X=x, Y=y)\\)\nContinuous PDF: \\(f(x, y) = \\frac{\\partial^2 F(x, y)}{\\partial x \\partial y}\\)\nIndependence can be modeled via CDF, PMF, and PDF. If the following hold, the joint distributions are independent:\n\nCDF: \\(F(x, y) = F(x)F(y)\\)\n\npmf: \\(p(x, y) = p(x)p(y)\\)\n\npdf: \\(f(x, y) = f(x)f(y)\\)\n\nProperties of the Joint PMF\n\n\\(p(x, y) \\geq 0\\)\n\\(\\sum\\limits_x \\sum\\limits_y p(x, y) = 1\\)\n\nMarginals build from the idea of the law of total probability.\n\nmarginal pmf for \\(X\\): \\(p_X(a) = \\sum\\limits_y p(a, y)\\)\n\nmarginal pmf for \\(Y\\): \\(p_Y(b) = \\sum\\limits_x p(x, b)\\)\n\n\nConditional PMF: \\(p_{X|Y}(x|y) = P(X=x|Y=y) = \\frac{P(X=x, Y=y)}{P(Y=y)}\\)\nProperties of the joint PDF\n\n\\(f(x, y) \\geq 0\\)\n\\(\\int\\limits_x \\int\\limits_y f(x, y) = 1\\)\n\n\nGeometrically, the probability can be thought of as volume under a surface defined by the pdf.\n\nMarginals build from the idea of the law of total probability.\n\nmarginal pdf for \\(X\\): \\(f_X(a) = \\int\\limits_y f(a, y)\\)\n\nmarginal pdf for \\(Y\\): \\(f_Y(b) = \\int\\limits_y p(x, b)\\)\n\n\nConditional PDF: \\(f_{X|Y}(x|y) = P(X \\leq x|Y \\leq y) = \\frac{P(X \\leq x, Y \\leq y)}{P(Y \\leq y)}\\)"
  },
  {
    "objectID": "review.html#covariance-correlation",
    "href": "review.html#covariance-correlation",
    "title": "Basic Review",
    "section": "Covariance & Correlation",
    "text": "Covariance & Correlation\nThe exact definition of covariance for discrete and continuous:\n\nDiscrete: \\(Cov(X, Y) = \\sum\\limits_x \\sum\\limits_y (x - \\mu_x)(y - \\mu_y) p(x, y)\\)\n\nContinuous: \\(Cov(X, Y) = \\int\\limits_x \\int\\limits_y (x - \\mu_x)(y - \\mu_y) f(x, y)dxdy\\)\n\n\nA better, generic way to calculate covariance:\n\\(Cov(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]\\)\nProof:\n\\(Cov(X, Y) = E[(X - E[X])(Y - E[Y])]\\)\n\\(= E[XY + XE[Y] - YE[X] + E[X]E[Y]]\\)\n\\(= E[XY] + E[X]E[Y] - E[Y]E[X] + E[X]E[Y]\\)\n\\(= E[XY] - E[X]E[Y]\\)\nIf \\(X\\) and \\(Y\\) are independent, covariance is 0\nCorrelation:\n\\(Corr(X, Y) = \\frac{Cov(X, Y)}{SD(X)SD(Y)}\\)\nIf \\(X\\) and \\(Y\\) are independent, correlation is 0"
  },
  {
    "objectID": "review.html#distributions",
    "href": "review.html#distributions",
    "title": "Basic Review",
    "section": "Distributions",
    "text": "Distributions\nTo showcase the distribution formulas in R, we’ll use the example of the normal function. However, most common distributions share the same blueprint as the functions we’ll call.\nWe can use the normal distribution in R via the following:\n\n\ndnorm: Returns a value from the distribution provided a location, mean, and standard deviation. Calling a specific value from a distribution has more meaning and applications within discrete distributions.\n\npnorm: Returns a value from the cumulative distribution function (cdf) provided a location, mean, and standard deviation. In other words, returns the area to the left of the given value from the normal cdf. Optional parameter of lower.tail=FALSE to get the area to the right of the given value.\n\nqnorm: Returns the value of the \\(p^{th}\\) quantile (Z-score) provided a quantile, mean, and standard deviation.\n\nrnorm: Returns a vector of normally distributed random variables provided size of desired vector, mean, and standard deviation.\n\n\n\n\n\n\n\nNote\n\n\n\nImport Libraries\n\n\n\nCode# import libraries\nlibrary(tidyverse)\n\n\n\n\n\n\n\n\nNote\n\n\n\ndnorm: value of the standard normal distribution pdf at x=0.\n\n\n\nCode# dnorm: value of the standard normal distribution pdf at x=0\ndnorm(x = 0, mean = 0, sd = 1)\n\n[1] 0.3989423\n\n\n\n\n\n\n\n\nNote\n\n\n\npnorm: amount of the standard normal distribution pdf to the left of x=0.\n\n\n\nCode# pnorm: amount of the standard normal distribution pdf to the left of x=0\npnorm(q = 0, mean = 0, sd = 1)\n\n[1] 0.5\n\n\n\n\n\n\n\n\nNote\n\n\n\nqnorm: find the 0.50 quantile of the standard normal distribution.\n\n\n\nCode# qnorm: find the 0.50 quantile of the standard normal distribution\nqnorm(p = 0.50, mean = 0, sd = 1)\n\n[1] 0\n\n\n\n\n\n\n\n\nNote\n\n\n\nrnorm: vector of 1000 random draws from the standard normal distribution.\n\n\n\nCode# rnorm: vector of 1000 random draws from the standard normal distribution\nnormal_vector &lt;- rnorm(n = 1000, mean = 0, sd = 1)\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe’ll illustrate rnorm by plotting the density distribution of the vector created.\n\n\n\nCodeggplot() + \n  geom_density(aes(normal_vector), fill = 'blue', alpha = 0.5) +\n  xlab('Randomly Drawn Values from the Standard Normal Distribution') +\n  ylab('Density')\n\n\n\n\n\n\n\nAs previously stated, many of the common distributions have these built in functionalities within R. For example, the exponential distribution has the functions dexp, pexp, qexp, and rexp. These follow a similar blueprint as to what is seen with the normal distribution functions above, but may contain slightly different nomenclature and parameters."
  },
  {
    "objectID": "review.html#helpful-functions",
    "href": "review.html#helpful-functions",
    "title": "Basic Review",
    "section": "Helpful Functions",
    "text": "Helpful Functions\nThe Apply Family\nA useful assortment of functions in R are available through the apply family:\n\napply\nlapply\nsapply\ntapply\nSample and Replicate\nThe sample and replicate functions are useful individually and when combined can be incredibly useful for statistical modeling.\nAdditionally useful in statistical modeling is the quantile function."
  }
]